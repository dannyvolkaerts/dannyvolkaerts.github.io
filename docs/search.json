[
  {
    "objectID": "sql_course/Bikestore_uitleg.html",
    "href": "sql_course/Bikestore_uitleg.html",
    "title": "De Bikestore",
    "section": "",
    "text": "Database schema\n\n\n\nBelangrijke informatie bij de bike store database\nZoals je in het diagram kunt zien, heeft de BikeStores voorbeeld database twee schema’s: verkoop en productie, en deze schema’s hebben negen tabellen. De tabel sales.stores bevat de winkelinformatie. Elke winkel heeft een winkelnaam, contactinformatie zoals telefoon en e-mail, en een adres inclusief straat, stad, staat en postcode. In de tabel sales.staffs wordt de essentiële informatie van het personeel opgeslagen, waaronder de voornaam en achternaam. Het bevat ook de communicatie-informatie zoals e-mail en telefoon.\nEen medewerker werkt in een winkel die wordt gespecificeerd door de waarde in de store_id kolom. Een winkel kan een of meer medewerkers hebben.\nEen medewerker rapporteert aan een filiaalmanager die gespecificeerd wordt door de waarde in de manager_id kolom. Als de waarde in de manager_id nul is, dan is het personeel de topmanager.\nAls een medewerker niet langer voor een winkel werkt, wordt de waarde in de kolom actief op nul gezet.\nDe tabel production.categories slaat de fietscategorieën op, zoals kinderfietsen, comfortfietsen en elektrische fietsen. De tabel production.brands slaat de merkinformatie van fietsen op, bijvoorbeeld Electra, Haro en Heller. De tabel production.products slaat productinformatie op, zoals naam, merk, categorie, modeljaar en catalogusprijs.\nElk product behoort tot een merk dat wordt gespecificeerd door de kolom brand_id. Een merk kan dus nul of veel producten hebben.\nElk product behoort ook tot een categorie die wordt gespecificeerd door de categorie_id kolom. Elke categorie kan ook nul of veel producten hebben. De tabel sales.customers slaat klantgegevens op, waaronder voornaam, achternaam, telefoon, e-mail, straat, stad, staat en postcode. In de tabel sales.orders worden de hoofdgegevens van de verkooporder opgeslagen, waaronder de klant, de status van de order, de besteldatum, de vereiste datum en de verzenddatum.\nDe tabel slaat ook de informatie op over waar de verkooptransactie is aangemaakt (winkel) en wie deze heeft aangemaakt (personeel). In de tabel sales.order_items worden de regelitems van een verkooporder opgeslagen. Elk regelitem hoort bij een verkooporder die wordt gespecificeerd door de order_id kolom.\nEen regelitem van een verkooporder bevat het product, de bestelhoeveelheid, de catalogusprijs en de korting.\nElke verkooporder heeft een rij in de tabel verkooporders. Een verkooporder heeft een of meer regelitems die zijn opgeslagen in de tabel sales.order_items.\nDe tabel production.stocks slaat de voorraadinformatie op, d.w.z. de hoeveelheid van een bepaald product in een specifieke winkel."
  },
  {
    "objectID": "sql_course/11_exercises_subqueries.html#de-tennisvereniging",
    "href": "sql_course/11_exercises_subqueries.html#de-tennisvereniging",
    "title": "Oefeningen subqueries",
    "section": "De Tennisvereniging",
    "text": "De Tennisvereniging"
  },
  {
    "objectID": "sql_course/11_exercises_subqueries.html#subqueries-h5",
    "href": "sql_course/11_exercises_subqueries.html#subqueries-h5",
    "title": "Oefeningen subqueries",
    "section": "SUBQUERIES (H5)",
    "text": "SUBQUERIES (H5)"
  },
  {
    "objectID": "sql_course/9_exercises_joins.html#de-tennisvereniging",
    "href": "sql_course/9_exercises_joins.html#de-tennisvereniging",
    "title": "Oefeningen hoofdstuk 3: tennis joins",
    "section": "De Tennisvereniging",
    "text": "De Tennisvereniging"
  },
  {
    "objectID": "sql_course/9_exercises_joins.html#joins-h3",
    "href": "sql_course/9_exercises_joins.html#joins-h3",
    "title": "Oefeningen hoofdstuk 3: tennis joins",
    "section": "JOINS (H3)",
    "text": "JOINS (H3)"
  },
  {
    "objectID": "sql_course/8_exercises.html#de-tennisvereniging",
    "href": "sql_course/8_exercises.html#de-tennisvereniging",
    "title": "Oefeningen hoofdstuk 1 en 2: tennis",
    "section": "De Tennisvereniging",
    "text": "De Tennisvereniging"
  },
  {
    "objectID": "sql_course/8_exercises.html#basis-h1h2",
    "href": "sql_course/8_exercises.html#basis-h1h2",
    "title": "Oefeningen hoofdstuk 1 en 2: tennis",
    "section": "BASIS (H1,H2)",
    "text": "BASIS (H1,H2)"
  },
  {
    "objectID": "sql_course/index_sql.html",
    "href": "sql_course/index_sql.html",
    "title": "SQL Course",
    "section": "",
    "text": "SQL, or Structured Query Language, is a standardized programming language used to manage and manipulate relational databases. Understanding SQL opens opportunities in data analysis, data engineering, and beyond, making it an essential skill for any aspiring data professional. It enables data professionals to derive meaningful insights from large datasets, perform data analyses, and input data into their machine learning models.\nSQL was developed in the 1970s and has since become the universal language for relational database management systems (RDBMS) like MySQL, PostgreSQL, SQLite, and Microsoft SQL Server. It is widely used in data science, software development, and business intelligence for tasks such as:\nData Retrieval: Extracting specific information from large datasets using SELECT statements.\nData Manipulation: Modifying data with commands like INSERT, UPDATE, and DELETE.\nData Definition: Creating and managing database structures with CREATE, ALTER, and DROP.\nData Analysis: Aggregating and analyzing data using functions like GROUP BY, HAVING, and COUNT.\nSQL’s simplicity and power make it an indispensable tool for professionals working with data, enabling efficient data management and analysis across industries."
  },
  {
    "objectID": "sql_course/index_sql.html#chapters",
    "href": "sql_course/index_sql.html#chapters",
    "title": "SQL Course",
    "section": "Chapters",
    "text": "Chapters\n\nIntroduction to SQL and Databases\nOverview of SQL, databases, and how they are used in data science.\n\n\n\nBasic SQL Queries  Learning SELECT statements and filtering data with WHERE clauses.\n\n\n\nWorking with Table Joins\nIntroduction to JOIN operations and their importance.\n\n\n\nAdvanced Data Manipulation\nUsing GROUP BY, HAVING, and aggregate functions.\n\n\n\nSubqueries, Nested Queries, and Views\nExtending queries with subqueries for more complex data retrieval.\n\n\n\nData Definition Language (DDL) & Data Manipulation Language (DML)\nUnderstanding how to create a database and insert, update, and delete data in the database.\n\n\n\nSQL and PowerBI\nExtracting insights from data by combining the power of SQL with PowerBI.\n\n\n\nExercises Chapter 1 and 2\nExercises for chapters 1 and 2.\n\n\n\nExercises Chapter 3\nExercises for chapter 3 - joins.\n\n\n\nExercises Chapter 4\nExercises for chapter 4 - GROUP BY, HAVING, etc.\n\n\n\nExercises Chapter 5\nExercises for chapter 5 - subqueries.\n\n\n\nExercises Chapter 6\nExercises for DDL - the bike shop."
  },
  {
    "objectID": "posts_main.html",
    "href": "posts_main.html",
    "title": "Articles",
    "section": "",
    "text": "Introduction to DuckDB\n\n\n\nDuckDB\n\n\nPython\n\n\nData Science\n\n\nAnalytics\n\n\n\nA comprehensive guide to DuckDB: an analytical database designed for seamless data analysis with Python\n\n\n\nDanny Volkaerts\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Danny Volkaerts",
    "section": "",
    "text": "Hi! Welcome to my digital space! As a data enthousiast I’m fueled by curiosity and motivated by the goal of turning data into actionable insights. Whether it’s through Machine Learning, data analysis, or data visualizations, I explore data in all its forms to unlock opportunities and drive innovation.\nHere, as a work in progress, I share my experiences, discoveries, and the lessons learned along the way with companies and fellow data enthusiasts. Dive into my courses, read through case studies, or reach out directly if you have any data related question!\n\n\n\n\nCatholic University of Leuven, Leuven, Belgium | Bio-engineering Sciences | Graduated 2010\n\n\n\n\n\nUC Leuven-Limburg Research & Expertise | Data Scientist | Present\nIMEC | Process Engineer | Until April 2018\n\n\n \n  \n   \n  \n    \n     LinkedIn"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Danny Volkaerts",
    "section": "",
    "text": "Catholic University of Leuven, Leuven, Belgium | Bio-engineering Sciences | Graduated 2010"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Danny Volkaerts",
    "section": "",
    "text": "UC Leuven-Limburg Research & Expertise | Data Scientist | Present\nIMEC | Process Engineer | Until April 2018"
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Courses",
    "section": "",
    "text": "Courses\n\nSQL Course: Learn the basics of SQL"
  },
  {
    "objectID": "sql_course/1_introduction.html#inleiding-tot-sql-and-databases",
    "href": "sql_course/1_introduction.html#inleiding-tot-sql-and-databases",
    "title": "Introductie SQL",
    "section": "Inleiding tot SQL and Databases",
    "text": "Inleiding tot SQL and Databases\n\nhttps://dannyvolkaerts.github.io/\nManier van werken: 1. EERD graag afdrukken of naast je houden op ander scherm 2. Interactie via Wooclap -&gt; ik ben mee met het verhaal of niet, eventueel enkele tussenvragen om te pijlen hoe ze mee zijn\nOefeningen via Azure Data Studio: New connection - Microsoft SQL Server\nExtentions links - MySQL -&gt; installeren"
  },
  {
    "objectID": "sql_course/1_introduction.html#sql-in-actie",
    "href": "sql_course/1_introduction.html#sql-in-actie",
    "title": "Introductie SQL",
    "section": "SQL in actie",
    "text": "SQL in actie"
  },
  {
    "objectID": "sql_course/7_powerbi.html#waarom-powerbi-en-sql---belang-van-data-intelligentie",
    "href": "sql_course/7_powerbi.html#waarom-powerbi-en-sql---belang-van-data-intelligentie",
    "title": "Integratie van SQL in PowerBI",
    "section": "Waarom PowerBI en SQL? - Belang van Data-intelligentie",
    "text": "Waarom PowerBI en SQL? - Belang van Data-intelligentie"
  },
  {
    "objectID": "sql_course/7_powerbi.html#concept-en-werkwijze",
    "href": "sql_course/7_powerbi.html#concept-en-werkwijze",
    "title": "Integratie van SQL in PowerBI",
    "section": "Concept en Werkwijze",
    "text": "Concept en Werkwijze"
  },
  {
    "objectID": "sql_course/7_powerbi.html#voorbeelden-van-powerbi-en-sql",
    "href": "sql_course/7_powerbi.html#voorbeelden-van-powerbi-en-sql",
    "title": "Integratie van SQL in PowerBI",
    "section": "Voorbeelden van PowerBI en SQL",
    "text": "Voorbeelden van PowerBI en SQL"
  },
  {
    "objectID": "sql_course/Tennis_uitleg.html",
    "href": "sql_course/Tennis_uitleg.html",
    "title": "De Tennisvereniging",
    "section": "",
    "text": "Database schema\n\n\n\nBelangrijke informatie bij de tennis database\nDe tennisvereniging is opgericht in 1970 en vanaf het begin wordt een aantal administratieve gegevens in een database opgeslagen. Deze database bestaat uit de volgende tabellen: spelers, teams, wedstrijden, boetes en bestuursleden.\n\nDe spelerstabel bevat gegevens over spelers die lid zijn van de tennisvereniging, zoals namen, adressen en geboortedatums. Toetreding tot de vereniging vindt altijd plaats op 1 januari van een bepaald jaar. Spelers kunnen dus niet midden in een jaar lid worden. De spelerstabel bevat geen historische gegevens. Als een speler zijn of haar lidmaatschap opzegt, verdwijnt hij of zij uit de tabel. Ook bij eventuele verhuizingen wordt het oude adres overschreven met het nieuwe adres, het oude adres wordt dus nergens bewaard.\n\nDe tennisvereniging kent twee soorten leden: recreatiespelers en wedstrijdspelers. De eerste groep speelt alleen onderlinge wedstrijden, dus geen wedstrijden tegen spelers van andere verenigingen. De resultaten van de onderlinge wedstrijden worden niet geregistreerd. Wedstrijdspelers spelen in teamverband tegen spelers van andere verenigingen. De resultaten van deze wedstrijden worden wel bijgehouden. Elke speler heeft een uniek nummer, ongeacht of deze wedstrijdspeler is of niet. Dit spelersnummer wordt door de vereniging uitgedeeld. Het is verplicht dat elke wedstrijdspeler bij de tennisbond geregistreerd staat.\n\nDe bond, die een nationaal instituut is, geeft elke wedstrijdspeler een uniek bondsnummer. Dit bondsnummer bestaat meestal uit cijfers, maar kan ook letters bevatten. Als een wedstrijdspeler geen wedstrijden meer speelt, maar recreatiespeler wordt, vervalt het bondsnummer. Let wel, recreatiespelers hebben dus geen bondsnummer, maar wel een spelersnummer.\n\nDe tennisvereniging heeft een aantal teams dat meedoet in competities. Voor elk team worden de aanvoerder en de divisie waarin het team op dat moment uitkomt, geregistreerd. De aanvoerder hoeft geen wedstrijden voor het team gespeeld te hebben. Het zou kunnen dat een bepaalde speler op een bepaald moment aanvoerder van twee of meer teams is. Ook in deze tabel wordt geen historie bijgehouden. Bij promotie of degradatie van een team naar een andere divisie wordt de geregistreerde divisie eenvoudigweg overschreven. Hetzelfde geldt voor de aanvoerder van een team: bij wisseling wordt het nummer van de oude aanvoerder overschreven.\n\nEen team bestaat uit een aantal spelers. Als een team tegen een team van een andere vereniging speelt, speelt elke speler van dat team een wedstrijd tegen een speler van het andere team (we gaan voor het gemak ervan uit dat wedstrijden waarbij koppels tegen elkaar spelen niet voorkomen). Het team waarvan de meeste spelers hun wedstrijd winnen is winnaar.\n\nEen team bestaat niet altijd uit dezelfde groep spelers. Bij ziekte of vakanties zijn soms invallers nodig. Een speler kan dus voor meerdere teams uitkomen. Als we spreken over ‘de spelers van een team’, dan bedoelen we dus de spelers die minstens één wedstrijd voor het team gespeeld hebben. Nogmaals, alleen spelers met een bondsnummer mogen officiële wedstrijden spelen.\n\nEen tenniswedstrijd is opgebouwd uit een aantal sets. Degene die de meeste sets heeft gewonnen is winnaar. Voor elke wedstrijd wordt vooraf bepaald bij hoeveel gewonnen sets de wedstrijd gewonnen is. Over het algemeen wordt de wedstrijd gestopt als een van de twee spelers twee of drie sets gewonnen heeft. Mogelijke eindstanden van een tenniswedstrijd zijn dus 2-1 of 2-0 als gespeeld wordt totdat een van de spelers twee sets gewonnen heeft (best of three), of 3-2, 3-1 of 3-0 als gespeeld wordt tot drie gewonnen sets (best of five). Een speler kan zijn of haar wedstrijd winnen of verliezen, gelijkspel is niet mogelijk. In de wedstrijdentabel wordt voor elke wedstrijd apart bijgehouden welke speler de wedstrijd heeft gespeeld en voor welk team. Tevens wordt geregistreerd hoeveel sets de speler heeft gewonnen en verloren. Hieruit is af te leiden of hij of zij de wedstrijd gewonnen heeft.\n\nVoor onreglementair gedrag van spelers (te late opkomst, agressief gedrag of niet verschijnen) worden door de bond boetes opgelegd. Boetes worden door de vereniging betaald. Na betaling worden ze in de boetestabel geregistreerd. Zolang een speler wedstrijden speelt, blijven alle boetes bewaard die voor hem of haar opgelegd en betaald zijn.\n\nAls een speler de vereniging verlaat, worden al zijn of haar gegevens in de vijf tabellen vernietigd. Als de vereniging een team terugtrekt, worden alle gegevens over dat team uit de teams- en wedstrijdentabel verwijderd. Als een wedstrijdspeler stopt met het spelen van wedstrijden en hij of zij dus weer recreant wordt, worden alle wedstrijd- en boetegegevens uit de desbetreffende tabellen verwijderd.\n\nSinds 1 januari 1990 wordt in de bestuursledentabel bijgehouden wie er in het bestuur zitten. Vier functies worden onderscheiden: voorzitter, penningmeester, secretaris en algemeen lid. Elk jaar op 1 januari wordt een nieuw bestuur gekozen. Wanneer een speler een bestuursfunctie bekleedt, worden de begin- en einddatum hiervan geregistreerd. Als iemand nog actief is, wordt er geen einddatum ingevuld.\n\n\n\nEnkele aandachtspunten\nIn het database schema willen we even wijzen op enkele punten:\n\nMerk op dat het spelersnummer een centrale rol in het schema heeft. Het is in één tabel de primaire sleutel en in de vier andere tabellen de externe sleutel (‘foreign key’).\n\nDe bestuursledentabel heeft een samengestelde primaire sleutel. In de figuur zie je dat aan de twee sleutelicoontjes, in de code staan er twee kolommen in de PRIMARY KEY.\n\nEen veelgemaakte fout is geen rekening houden met het feit dat het spelersnummer in de teamstabel het nummer is van de kapitein van een team. Om te weten wie effectief wedstrijden gespeeld heeft, moet je in de wedstrijdentabel het spelersnummer gebruiken."
  },
  {
    "objectID": "sql_course/10_exercises_advanced.html#de-tennisvereniging",
    "href": "sql_course/10_exercises_advanced.html#de-tennisvereniging",
    "title": "Oefeningen advanced",
    "section": "De Tennisvereniging",
    "text": "De Tennisvereniging"
  },
  {
    "objectID": "sql_course/12_exercise_DDL.html#de-fietsenwinkel",
    "href": "sql_course/12_exercise_DDL.html#de-fietsenwinkel",
    "title": "Oefeningen DDL - Fietsenwinkel",
    "section": "De fietsenwinkel",
    "text": "De fietsenwinkel"
  },
  {
    "objectID": "sql_course/12_exercise_DDL.html#ddl",
    "href": "sql_course/12_exercise_DDL.html#ddl",
    "title": "Oefeningen DDL - Fietsenwinkel",
    "section": "DDL",
    "text": "DDL"
  },
  {
    "objectID": "posts_main/DuckDB/index.html",
    "href": "posts_main/DuckDB/index.html",
    "title": "Introduction to DuckDB",
    "section": "",
    "text": "This document provides a comprehensive overview of DuckDB, exploring its fundamental concepts, key strengths, and practical applications. We’ll demonstrate its capabilities through a detailed example using the Titanic dataset, imported directly from Kaggle using their API.\n\n\nDuckDB is a high-performance analytical database specifically engineered to operate seamlessly within your applications. By default, DuckDB operates with an in-memory database that maintains global state within the Python module when utilizing duckdb.sql(). This design choice means that no persistent tables are stored on disk unless explicitly configured otherwise.\nHowever, DuckDB also provides the flexibility to establish connections with persistent databases by specifying a filename through duckdb.connect(dbname). All data written to these connections remains persistent and can be reloaded by reconnecting to the same file, whether from Python or other DuckDB clients.\nDuckDB demonstrates remarkable versatility through its support for an extensive range of Client APIs, including: * C - Low-level systems programming * C++ - High-performance applications * CLI (Command Line Interface) - Interactive database operations * Dart - Mobile and web application development * Go - Concurrent and scalable applications * Java (JDBC) - Enterprise Java applications * Julia - High-performance scientific computing * Node.js - Server-side JavaScript applications * ODBC - Universal database connectivity * Python - Data science and analytics workflows * R - Statistical computing and data analysis * Rust - Systems programming with memory safety * Swift - iOS and macOS applications * WebAssembly (Wasm) - Browser-based applications\nThe database engine provides comprehensive SQL functionality, encompassing essential statements such as SELECT, INSERT, CREATE TABLE, ALTER TABLE, DELETE, UPDATE, and COPY. It includes advanced query syntax capabilities, diverse data types (Array, Boolean, Date, Numeric, Text, Timestamp), sophisticated expressions, and an extensive collection of functions including Aggregate, Date, Numeric, Text, and Window Functions.\n\n\n\nDuckDB’s distinctive strengths establish it as a powerful tool for data analysis and processing, particularly when integrated with Python ecosystems:\n\n\nDuckDB excels in its ability to ingest data from a diverse array of formats, supporting both disk-based and in-memory operations. The database natively supports CSV, Parquet, and JSON file formats. One of its most compelling features is the capability to query these files directly using SQL as if they were traditional database tables. For instance, you can execute SELECT * FROM 'example.csv' without any preliminary data loading steps.\n\n\n\nDuckDB provides native support for querying Pandas DataFrames, Polars DataFrames, and PyArrow tables directly. This integration represents a significant advantage, enabling analysts to leverage SQL queries on existing Python data structures without requiring conversion or loading into separate database systems. It’s important to note that these direct queries operate in read-only mode.\n\n\n\nQuery results can be efficiently transformed into various Python objects and popular data formats. The system supports seamless conversion to Pandas DataFrames (.df()), Polars DataFrames (.pl()), Arrow Tables (.arrow()), and NumPy Arrays (.fetchnumpy()), facilitating smooth workflow integration across different data processing libraries.\n\n\n\nDuckDB offers users the choice between in-memory databases for rapid, temporary analyses and persistent storage solutions for data preservation. This flexibility allows for optimal performance tuning based on specific use case requirements.\n\n\n\nQuery results (Relation objects) can be written directly to disk in formats such as Parquet and CSV, or you can utilize the SQL COPY statement for more complex export operations. This functionality streamlines data pipeline workflows.\n\n\n\nThe duckdb module and connection objects (duckdb.connect()) support identical methods, providing implementation flexibility. For package development, connection objects are recommended to prevent potential conflicts with shared global in-memory databases.\n\n\n\nDuckDB supports the installation and loading of extensions, such as the spatial extension for geospatial data analysis or httpfs for accessing HTTP(S) and S3-based files. The platform also supports community-developed extensions through the repository=\"community\" parameter.\n\n\n\nQueries return Relation objects, which serve as symbolic representations of the query. The actual execution occurs only when results are explicitly requested or displayed, contributing to overall system efficiency through lazy evaluation.\n\n\n\n\nThis comprehensive example demonstrates how to leverage DuckDB’s capabilities by importing the famous Titanic dataset directly from Kaggle using their API, then performing sophisticated data analysis using SQL queries.\n\n\nBefore proceeding, ensure you have the following requirements:\n\nPython 3.9 or newer - DuckDB requires a modern Python version\nKaggle API credentials - You’ll need a Kaggle account and API key\nRequired packages - Install via pip install duckdb kaggle pandas\n\n\n\n\nFirst, let’s set up the Kaggle API to download the Titanic dataset:\n#| echo: true\n#| output: false\nimport os\nimport zipfile\nimport pandas as pd\nimport duckdb\nfrom kaggle.api.kaggle_api_extended import KaggleApi\n\n# Initialize and authenticate Kaggle API\n# Note: Make sure you have ~/.kaggle/kaggle.json with your credentials\n# or set KAGGLE_USERNAME and KAGGLE_KEY environment variables\napi = KaggleApi()\napi.authenticate()\n\n# Download the Titanic dataset from Kaggle\nprint(\"Downloading Titanic dataset from Kaggle...\")\napi.competition_download_files('titanic', path='.', quiet=False)\n\n# Extract the downloaded zip file\nwith zipfile.ZipFile('titanic.zip', 'r') as zip_ref:\n    zip_ref.extractall('.')\n\nprint(\"Dataset downloaded and extracted successfully!\")\n\n# List the extracted files\nextracted_files = [f for f in os.listdir('.') if f.endswith('.csv')]\nprint(f\"Available CSV files: {extracted_files}\")\n\n\n\nNow let’s perform comprehensive analysis using DuckDB’s powerful SQL capabilities:\n#| echo: true\nimport duckdb\n\n# Connect to DuckDB (in-memory database)\nconn = duckdb.connect(':memory:')\n\nprint(\"=== TITANIC DATASET ANALYSIS WITH DUCKDB ===\\n\")\n\n# Query 1: Basic dataset overview\nprint(\"1. Dataset Overview - First 5 rows:\")\noverview_query = \"\"\"\n    SELECT * \n    FROM 'train.csv' \n    LIMIT 5\n\"\"\"\nresult = conn.execute(overview_query)\nresult.df().head()\n\nprint(\"\\n2. Dataset Dimensions and Basic Statistics:\")\nstats_query = \"\"\"\n    SELECT \n        COUNT(*) as total_passengers,\n        COUNT(DISTINCT Pclass) as passenger_classes,\n        COUNT(DISTINCT Sex) as gender_categories,\n        COUNT(DISTINCT Embarked) as embarkation_ports,\n        AVG(Age) as average_age,\n        MIN(Age) as youngest_passenger,\n        MAX(Age) as oldest_passenger,\n        AVG(Fare) as average_fare\n    FROM 'train.csv'\n\"\"\"\nstats_result = conn.execute(stats_query).df()\nprint(stats_result.to_string(index=False))\n\nprint(\"\\n3. Survival Analysis by Demographics:\")\nsurvival_demographics = \"\"\"\n    SELECT \n        Sex,\n        Pclass,\n        COUNT(*) as total_passengers,\n        SUM(Survived) as survivors,\n        ROUND(AVG(Survived) * 100, 2) as survival_rate_percent,\n        ROUND(AVG(Age), 1) as avg_age,\n        ROUND(AVG(Fare), 2) as avg_fare\n    FROM 'train.csv'\n    WHERE Age IS NOT NULL\n    GROUP BY Sex, Pclass\n    ORDER BY Sex, Pclass\n\"\"\"\ndemographics_result = conn.execute(survival_demographics).df()\nprint(demographics_result.to_string(index=False))\n\nprint(\"\\n4. Advanced Analysis: Survival by Age Groups:\")\nage_group_analysis = \"\"\"\n    SELECT \n        CASE \n            WHEN Age &lt; 18 THEN 'Child (0-17)'\n            WHEN Age &gt;= 18 AND Age &lt; 35 THEN 'Young Adult (18-34)'\n            WHEN Age &gt;= 35 AND Age &lt; 55 THEN 'Middle Age (35-54)'\n            WHEN Age &gt;= 55 THEN 'Senior (55+)'\n            ELSE 'Unknown'\n        END as age_group,\n        COUNT(*) as total_passengers,\n        SUM(Survived) as survivors,\n        ROUND(AVG(Survived) * 100, 2) as survival_rate_percent,\n        ROUND(AVG(Fare), 2) as avg_fare_paid\n    FROM 'train.csv'\n    WHERE Age IS NOT NULL\n    GROUP BY age_group\n    ORDER BY \n        CASE \n            WHEN age_group = 'Child (0-17)' THEN 1\n            WHEN age_group = 'Young Adult (18-34)' THEN 2\n            WHEN age_group = 'Middle Age (35-54)' THEN 3\n            WHEN age_group = 'Senior (55+)' THEN 4\n            ELSE 5\n        END\n\"\"\"\nage_analysis_result = conn.execute(age_group_analysis).df()\nprint(age_analysis_result.to_string(index=False))\n\nprint(\"\\n5. Family Size Impact on Survival:\")\nfamily_analysis = \"\"\"\n    SELECT \n        (SibSp + Parch) as family_size,\n        CASE \n            WHEN (SibSp + Parch) = 0 THEN 'Alone'\n            WHEN (SibSp + Parch) BETWEEN 1 AND 3 THEN 'Small Family (1-3)'\n            WHEN (SibSp + Parch) BETWEEN 4 AND 6 THEN 'Medium Family (4-6)'\n            ELSE 'Large Family (7+)'\n        END as family_category,\n        COUNT(*) as passenger_count,\n        SUM(Survived) as survivors,\n        ROUND(AVG(Survived) * 100, 2) as survival_rate_percent\n    FROM 'train.csv'\n    GROUP BY family_size, family_category\n    ORDER BY family_size\n\"\"\"\nfamily_result = conn.execute(family_analysis).df()\nprint(family_result.to_string(index=False))\n\nprint(\"\\n6. Port of Embarkation Analysis:\")\nembarkation_analysis = \"\"\"\n    SELECT \n        CASE \n            WHEN Embarked = 'C' THEN 'Cherbourg'\n            WHEN Embarked = 'Q' THEN 'Queenstown'\n            WHEN Embarked = 'S' THEN 'Southampton'\n            ELSE 'Unknown'\n        END as port_of_embarkation,\n        COUNT(*) as passenger_count,\n        SUM(Survived) as survivors,\n        ROUND(AVG(Survived) * 100, 2) as survival_rate_percent,\n        ROUND(AVG(Fare), 2) as avg_fare,\n        ROUND(AVG(Age), 1) as avg_age\n    FROM 'train.csv'\n    WHERE Embarked IS NOT NULL\n    GROUP BY Embarked, port_of_embarkation\n    ORDER BY survival_rate_percent DESC\n\"\"\"\nembarkation_result = conn.execute(embarkation_analysis).df()\nprint(embarkation_result.to_string(index=False))\n\n# Demonstrate DuckDB's ability to export results\nprint(\"\\n7. Exporting Analysis Results:\")\n# Export the demographic analysis to a new CSV file\nconn.execute(\"\"\"\n    COPY (\n        SELECT \n            Sex,\n            Pclass,\n            COUNT(*) as total_passengers,\n            SUM(Survived) as survivors,\n            ROUND(AVG(Survived) * 100, 2) as survival_rate_percent\n        FROM 'train.csv'\n        GROUP BY Sex, Pclass\n        ORDER BY Sex, Pclass\n    ) TO 'titanic_survival_analysis.csv' (HEADER, DELIMITER ',')\n\"\"\")\nprint(\"Results exported to 'titanic_survival_analysis.csv'\")\n\n# Demonstrate conversion to different Python data structures\nprint(\"\\n8. Converting Results to Different Data Structures:\")\nsample_result = conn.execute(\"SELECT * FROM 'train.csv' LIMIT 3\")\n\n# Convert to Pandas DataFrame\npandas_df = sample_result.df()\nprint(f\"Pandas DataFrame shape: {pandas_df.shape}\")\n\n# Convert to Arrow Table\narrow_table = sample_result.arrow()\nprint(f\"Arrow Table schema: {arrow_table.schema}\")\n\n# Convert to NumPy array (for numerical columns only)\nnumeric_result = conn.execute(\"SELECT Age, Fare FROM 'train.csv' WHERE Age IS NOT NULL LIMIT 5\")\nnumpy_array = numeric_result.fetchnumpy()\nprint(f\"NumPy arrays: {list(numpy_array.keys())}\")\n\n# Close the connection\nconn.close()\nprint(\"\\nAnalysis complete! DuckDB connection closed.\")\n\n\n\nThis example demonstrates several powerful DuckDB features:\n\nDirect CSV querying without loading data into memory first\nComplex SQL analytics with window functions and case statements\nEfficient data type handling and NULL value management\nSeamless integration with Python data science ecosystem\nMultiple export formats and data structure conversions\nPerformance optimization through lazy evaluation\n\nThe combination of Kaggle’s API and DuckDB’s analytical capabilities showcases how modern data science workflows can be streamlined and made more efficient. DuckDB’s SQL-first approach allows for complex analytical queries while maintaining the flexibility to integrate with existing Python data science libraries.\n\n\n\n#| echo: true\n#| output: false\n# Clean up downloaded files (optional)\nimport os\nfiles_to_remove = ['titanic.zip', 'train.csv', 'test.csv', 'gender_submission.csv', 'titanic_survival_analysis.csv']\nfor file in files_to_remove:\n    if os.path.exists(file):\n        os.remove(file)\n        print(f\"Removed: {file}\")\nThis comprehensive example illustrates how DuckDB can serve as a powerful analytical engine for data science workflows, combining the familiarity of SQL with the flexibility of Python data processing libraries."
  },
  {
    "objectID": "posts_main/DuckDB/index.html#what-is-duckdb",
    "href": "posts_main/DuckDB/index.html#what-is-duckdb",
    "title": "Introduction to DuckDB",
    "section": "",
    "text": "DuckDB is a high-performance analytical database specifically engineered to operate seamlessly within your applications. By default, DuckDB operates with an in-memory database that maintains global state within the Python module when utilizing duckdb.sql(). This design choice means that no persistent tables are stored on disk unless explicitly configured otherwise.\nHowever, DuckDB also provides the flexibility to establish connections with persistent databases by specifying a filename through duckdb.connect(dbname). All data written to these connections remains persistent and can be reloaded by reconnecting to the same file, whether from Python or other DuckDB clients.\nDuckDB demonstrates remarkable versatility through its support for an extensive range of Client APIs, including: * C - Low-level systems programming * C++ - High-performance applications * CLI (Command Line Interface) - Interactive database operations * Dart - Mobile and web application development * Go - Concurrent and scalable applications * Java (JDBC) - Enterprise Java applications * Julia - High-performance scientific computing * Node.js - Server-side JavaScript applications * ODBC - Universal database connectivity * Python - Data science and analytics workflows * R - Statistical computing and data analysis * Rust - Systems programming with memory safety * Swift - iOS and macOS applications * WebAssembly (Wasm) - Browser-based applications\nThe database engine provides comprehensive SQL functionality, encompassing essential statements such as SELECT, INSERT, CREATE TABLE, ALTER TABLE, DELETE, UPDATE, and COPY. It includes advanced query syntax capabilities, diverse data types (Array, Boolean, Date, Numeric, Text, Timestamp), sophisticated expressions, and an extensive collection of functions including Aggregate, Date, Numeric, Text, and Window Functions."
  },
  {
    "objectID": "posts_main/DuckDB/index.html#what-makes-duckdb-exceptional",
    "href": "posts_main/DuckDB/index.html#what-makes-duckdb-exceptional",
    "title": "Introduction to DuckDB",
    "section": "",
    "text": "DuckDB’s distinctive strengths establish it as a powerful tool for data analysis and processing, particularly when integrated with Python ecosystems:\n\n\nDuckDB excels in its ability to ingest data from a diverse array of formats, supporting both disk-based and in-memory operations. The database natively supports CSV, Parquet, and JSON file formats. One of its most compelling features is the capability to query these files directly using SQL as if they were traditional database tables. For instance, you can execute SELECT * FROM 'example.csv' without any preliminary data loading steps.\n\n\n\nDuckDB provides native support for querying Pandas DataFrames, Polars DataFrames, and PyArrow tables directly. This integration represents a significant advantage, enabling analysts to leverage SQL queries on existing Python data structures without requiring conversion or loading into separate database systems. It’s important to note that these direct queries operate in read-only mode.\n\n\n\nQuery results can be efficiently transformed into various Python objects and popular data formats. The system supports seamless conversion to Pandas DataFrames (.df()), Polars DataFrames (.pl()), Arrow Tables (.arrow()), and NumPy Arrays (.fetchnumpy()), facilitating smooth workflow integration across different data processing libraries.\n\n\n\nDuckDB offers users the choice between in-memory databases for rapid, temporary analyses and persistent storage solutions for data preservation. This flexibility allows for optimal performance tuning based on specific use case requirements.\n\n\n\nQuery results (Relation objects) can be written directly to disk in formats such as Parquet and CSV, or you can utilize the SQL COPY statement for more complex export operations. This functionality streamlines data pipeline workflows.\n\n\n\nThe duckdb module and connection objects (duckdb.connect()) support identical methods, providing implementation flexibility. For package development, connection objects are recommended to prevent potential conflicts with shared global in-memory databases.\n\n\n\nDuckDB supports the installation and loading of extensions, such as the spatial extension for geospatial data analysis or httpfs for accessing HTTP(S) and S3-based files. The platform also supports community-developed extensions through the repository=\"community\" parameter.\n\n\n\nQueries return Relation objects, which serve as symbolic representations of the query. The actual execution occurs only when results are explicitly requested or displayed, contributing to overall system efficiency through lazy evaluation."
  },
  {
    "objectID": "posts_main/DuckDB/index.html#practical-example-analyzing-the-titanic-dataset-with-duckdb-and-kaggle-api",
    "href": "posts_main/DuckDB/index.html#practical-example-analyzing-the-titanic-dataset-with-duckdb-and-kaggle-api",
    "title": "Introduction to DuckDB",
    "section": "",
    "text": "This comprehensive example demonstrates how to leverage DuckDB’s capabilities by importing the famous Titanic dataset directly from Kaggle using their API, then performing sophisticated data analysis using SQL queries.\n\n\nBefore proceeding, ensure you have the following requirements:\n\nPython 3.9 or newer - DuckDB requires a modern Python version\nKaggle API credentials - You’ll need a Kaggle account and API key\nRequired packages - Install via pip install duckdb kaggle pandas\n\n\n\n\nFirst, let’s set up the Kaggle API to download the Titanic dataset:\n#| echo: true\n#| output: false\nimport os\nimport zipfile\nimport pandas as pd\nimport duckdb\nfrom kaggle.api.kaggle_api_extended import KaggleApi\n\n# Initialize and authenticate Kaggle API\n# Note: Make sure you have ~/.kaggle/kaggle.json with your credentials\n# or set KAGGLE_USERNAME and KAGGLE_KEY environment variables\napi = KaggleApi()\napi.authenticate()\n\n# Download the Titanic dataset from Kaggle\nprint(\"Downloading Titanic dataset from Kaggle...\")\napi.competition_download_files('titanic', path='.', quiet=False)\n\n# Extract the downloaded zip file\nwith zipfile.ZipFile('titanic.zip', 'r') as zip_ref:\n    zip_ref.extractall('.')\n\nprint(\"Dataset downloaded and extracted successfully!\")\n\n# List the extracted files\nextracted_files = [f for f in os.listdir('.') if f.endswith('.csv')]\nprint(f\"Available CSV files: {extracted_files}\")\n\n\n\nNow let’s perform comprehensive analysis using DuckDB’s powerful SQL capabilities:\n#| echo: true\nimport duckdb\n\n# Connect to DuckDB (in-memory database)\nconn = duckdb.connect(':memory:')\n\nprint(\"=== TITANIC DATASET ANALYSIS WITH DUCKDB ===\\n\")\n\n# Query 1: Basic dataset overview\nprint(\"1. Dataset Overview - First 5 rows:\")\noverview_query = \"\"\"\n    SELECT * \n    FROM 'train.csv' \n    LIMIT 5\n\"\"\"\nresult = conn.execute(overview_query)\nresult.df().head()\n\nprint(\"\\n2. Dataset Dimensions and Basic Statistics:\")\nstats_query = \"\"\"\n    SELECT \n        COUNT(*) as total_passengers,\n        COUNT(DISTINCT Pclass) as passenger_classes,\n        COUNT(DISTINCT Sex) as gender_categories,\n        COUNT(DISTINCT Embarked) as embarkation_ports,\n        AVG(Age) as average_age,\n        MIN(Age) as youngest_passenger,\n        MAX(Age) as oldest_passenger,\n        AVG(Fare) as average_fare\n    FROM 'train.csv'\n\"\"\"\nstats_result = conn.execute(stats_query).df()\nprint(stats_result.to_string(index=False))\n\nprint(\"\\n3. Survival Analysis by Demographics:\")\nsurvival_demographics = \"\"\"\n    SELECT \n        Sex,\n        Pclass,\n        COUNT(*) as total_passengers,\n        SUM(Survived) as survivors,\n        ROUND(AVG(Survived) * 100, 2) as survival_rate_percent,\n        ROUND(AVG(Age), 1) as avg_age,\n        ROUND(AVG(Fare), 2) as avg_fare\n    FROM 'train.csv'\n    WHERE Age IS NOT NULL\n    GROUP BY Sex, Pclass\n    ORDER BY Sex, Pclass\n\"\"\"\ndemographics_result = conn.execute(survival_demographics).df()\nprint(demographics_result.to_string(index=False))\n\nprint(\"\\n4. Advanced Analysis: Survival by Age Groups:\")\nage_group_analysis = \"\"\"\n    SELECT \n        CASE \n            WHEN Age &lt; 18 THEN 'Child (0-17)'\n            WHEN Age &gt;= 18 AND Age &lt; 35 THEN 'Young Adult (18-34)'\n            WHEN Age &gt;= 35 AND Age &lt; 55 THEN 'Middle Age (35-54)'\n            WHEN Age &gt;= 55 THEN 'Senior (55+)'\n            ELSE 'Unknown'\n        END as age_group,\n        COUNT(*) as total_passengers,\n        SUM(Survived) as survivors,\n        ROUND(AVG(Survived) * 100, 2) as survival_rate_percent,\n        ROUND(AVG(Fare), 2) as avg_fare_paid\n    FROM 'train.csv'\n    WHERE Age IS NOT NULL\n    GROUP BY age_group\n    ORDER BY \n        CASE \n            WHEN age_group = 'Child (0-17)' THEN 1\n            WHEN age_group = 'Young Adult (18-34)' THEN 2\n            WHEN age_group = 'Middle Age (35-54)' THEN 3\n            WHEN age_group = 'Senior (55+)' THEN 4\n            ELSE 5\n        END\n\"\"\"\nage_analysis_result = conn.execute(age_group_analysis).df()\nprint(age_analysis_result.to_string(index=False))\n\nprint(\"\\n5. Family Size Impact on Survival:\")\nfamily_analysis = \"\"\"\n    SELECT \n        (SibSp + Parch) as family_size,\n        CASE \n            WHEN (SibSp + Parch) = 0 THEN 'Alone'\n            WHEN (SibSp + Parch) BETWEEN 1 AND 3 THEN 'Small Family (1-3)'\n            WHEN (SibSp + Parch) BETWEEN 4 AND 6 THEN 'Medium Family (4-6)'\n            ELSE 'Large Family (7+)'\n        END as family_category,\n        COUNT(*) as passenger_count,\n        SUM(Survived) as survivors,\n        ROUND(AVG(Survived) * 100, 2) as survival_rate_percent\n    FROM 'train.csv'\n    GROUP BY family_size, family_category\n    ORDER BY family_size\n\"\"\"\nfamily_result = conn.execute(family_analysis).df()\nprint(family_result.to_string(index=False))\n\nprint(\"\\n6. Port of Embarkation Analysis:\")\nembarkation_analysis = \"\"\"\n    SELECT \n        CASE \n            WHEN Embarked = 'C' THEN 'Cherbourg'\n            WHEN Embarked = 'Q' THEN 'Queenstown'\n            WHEN Embarked = 'S' THEN 'Southampton'\n            ELSE 'Unknown'\n        END as port_of_embarkation,\n        COUNT(*) as passenger_count,\n        SUM(Survived) as survivors,\n        ROUND(AVG(Survived) * 100, 2) as survival_rate_percent,\n        ROUND(AVG(Fare), 2) as avg_fare,\n        ROUND(AVG(Age), 1) as avg_age\n    FROM 'train.csv'\n    WHERE Embarked IS NOT NULL\n    GROUP BY Embarked, port_of_embarkation\n    ORDER BY survival_rate_percent DESC\n\"\"\"\nembarkation_result = conn.execute(embarkation_analysis).df()\nprint(embarkation_result.to_string(index=False))\n\n# Demonstrate DuckDB's ability to export results\nprint(\"\\n7. Exporting Analysis Results:\")\n# Export the demographic analysis to a new CSV file\nconn.execute(\"\"\"\n    COPY (\n        SELECT \n            Sex,\n            Pclass,\n            COUNT(*) as total_passengers,\n            SUM(Survived) as survivors,\n            ROUND(AVG(Survived) * 100, 2) as survival_rate_percent\n        FROM 'train.csv'\n        GROUP BY Sex, Pclass\n        ORDER BY Sex, Pclass\n    ) TO 'titanic_survival_analysis.csv' (HEADER, DELIMITER ',')\n\"\"\")\nprint(\"Results exported to 'titanic_survival_analysis.csv'\")\n\n# Demonstrate conversion to different Python data structures\nprint(\"\\n8. Converting Results to Different Data Structures:\")\nsample_result = conn.execute(\"SELECT * FROM 'train.csv' LIMIT 3\")\n\n# Convert to Pandas DataFrame\npandas_df = sample_result.df()\nprint(f\"Pandas DataFrame shape: {pandas_df.shape}\")\n\n# Convert to Arrow Table\narrow_table = sample_result.arrow()\nprint(f\"Arrow Table schema: {arrow_table.schema}\")\n\n# Convert to NumPy array (for numerical columns only)\nnumeric_result = conn.execute(\"SELECT Age, Fare FROM 'train.csv' WHERE Age IS NOT NULL LIMIT 5\")\nnumpy_array = numeric_result.fetchnumpy()\nprint(f\"NumPy arrays: {list(numpy_array.keys())}\")\n\n# Close the connection\nconn.close()\nprint(\"\\nAnalysis complete! DuckDB connection closed.\")\n\n\n\nThis example demonstrates several powerful DuckDB features:\n\nDirect CSV querying without loading data into memory first\nComplex SQL analytics with window functions and case statements\nEfficient data type handling and NULL value management\nSeamless integration with Python data science ecosystem\nMultiple export formats and data structure conversions\nPerformance optimization through lazy evaluation\n\nThe combination of Kaggle’s API and DuckDB’s analytical capabilities showcases how modern data science workflows can be streamlined and made more efficient. DuckDB’s SQL-first approach allows for complex analytical queries while maintaining the flexibility to integrate with existing Python data science libraries.\n\n\n\n#| echo: true\n#| output: false\n# Clean up downloaded files (optional)\nimport os\nfiles_to_remove = ['titanic.zip', 'train.csv', 'test.csv', 'gender_submission.csv', 'titanic_survival_analysis.csv']\nfor file in files_to_remove:\n    if os.path.exists(file):\n        os.remove(file)\n        print(f\"Removed: {file}\")\nThis comprehensive example illustrates how DuckDB can serve as a powerful analytical engine for data science workflows, combining the familiarity of SQL with the flexibility of Python data processing libraries."
  }
]