[
  {
    "objectID": "posts_main/Garmin_Competition_App/index.html",
    "href": "posts_main/Garmin_Competition_App/index.html",
    "title": "Interactive Garmin Mini Competition Dashboard",
    "section": "",
    "text": "This interactive dashboard showcases fitness and activity data through a comprehensive Shiny application. The app provides various visualizations and analytics tools to explore patterns in Garmin fitness tracker data from a mini competition.\n\n\n\nInteractive Data Exploration: Dynamic filtering and visualization options\nPerformance Analytics: Track and compare fitness metrics\nUser-Friendly Interface: Intuitive design for easy navigation\nReal-Time Updates: Live data visualization capabilities"
  },
  {
    "objectID": "posts_main/Garmin_Competition_App/index.html#about-this-application",
    "href": "posts_main/Garmin_Competition_App/index.html#about-this-application",
    "title": "Interactive Garmin Mini Competition Dashboard",
    "section": "",
    "text": "This interactive dashboard showcases fitness and activity data through a comprehensive Shiny application. The app provides various visualizations and analytics tools to explore patterns in Garmin fitness tracker data from a mini competition.\n\n\n\nInteractive Data Exploration: Dynamic filtering and visualization options\nPerformance Analytics: Track and compare fitness metrics\nUser-Friendly Interface: Intuitive design for easy navigation\nReal-Time Updates: Live data visualization capabilities"
  },
  {
    "objectID": "posts_main/Garmin_Competition_App/index.html#live-application",
    "href": "posts_main/Garmin_Competition_App/index.html#live-application",
    "title": "Interactive Garmin Mini Competition Dashboard",
    "section": "Live Application",
    "text": "Live Application\nExplore the interactive dashboard below. The application is fully functional and hosted on our Posit Connect server:\n\n \n\n\n ðŸ”— Open in New Tab"
  },
  {
    "objectID": "posts_main/Garmin_Competition_App/index.html#technical-implementation",
    "href": "posts_main/Garmin_Competition_App/index.html#technical-implementation",
    "title": "Interactive Garmin Mini Competition Dashboard",
    "section": "Technical Implementation",
    "text": "Technical Implementation\nThis Shiny application demonstrates several key technologies:\n\nR Shiny: Reactive web application framework\nInteractive Visualizations: Dynamic plots and charts\nData Processing: Real-time data manipulation and analysis\nResponsive Design: Optimized for various screen sizes"
  },
  {
    "objectID": "posts_main/Garmin_Competition_App/index.html#learning-objectives",
    "href": "posts_main/Garmin_Competition_App/index.html#learning-objectives",
    "title": "Interactive Garmin Mini Competition Dashboard",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy exploring this application, you can learn about:\n\nShiny Architecture: Understanding reactive programming concepts\nData Visualization: Creating engaging and informative charts\nUser Interface Design: Building intuitive web applications\nPerformance Optimization: Efficient data handling techniques"
  },
  {
    "objectID": "posts_main/Garmin_Competition_App/index.html#getting-started",
    "href": "posts_main/Garmin_Competition_App/index.html#getting-started",
    "title": "Interactive Garmin Mini Competition Dashboard",
    "section": "Getting Started",
    "text": "Getting Started\nSimply interact with the application above to:\n\nFilter data by different criteria\nExplore various visualization options\n\nAnalyze patterns in the fitness data\nCompare performance metrics\n\nThe application is fully interactive - click, filter, and explore to discover insights in the data!\n\nThis application is hosted on Posit Connect and demonstrates the power of R Shiny for creating interactive data applications."
  },
  {
    "objectID": "sql_course/Bikestore_uitleg.html",
    "href": "sql_course/Bikestore_uitleg.html",
    "title": "De Bikestore",
    "section": "",
    "text": "Database schema\n\n\n\nBelangrijke informatie bij de bike store database\nZoals je in het diagram kunt zien, heeft de BikeStores voorbeeld database twee schemaâ€™s: verkoop en productie, en deze schemaâ€™s hebben negen tabellen. De tabel sales.stores bevat de winkelinformatie. Elke winkel heeft een winkelnaam, contactinformatie zoals telefoon en e-mail, en een adres inclusief straat, stad, staat en postcode. In de tabel sales.staffs wordt de essentiÃ«le informatie van het personeel opgeslagen, waaronder de voornaam en achternaam. Het bevat ook de communicatie-informatie zoals e-mail en telefoon.\nEen medewerker werkt in een winkel die wordt gespecificeerd door de waarde in de store_id kolom. Een winkel kan een of meer medewerkers hebben.\nEen medewerker rapporteert aan een filiaalmanager die gespecificeerd wordt door de waarde in de manager_id kolom. Als de waarde in de manager_id nul is, dan is het personeel de topmanager.\nAls een medewerker niet langer voor een winkel werkt, wordt de waarde in de kolom actief op nul gezet.\nDe tabel production.categories slaat de fietscategorieÃ«n op, zoals kinderfietsen, comfortfietsen en elektrische fietsen. De tabel production.brands slaat de merkinformatie van fietsen op, bijvoorbeeld Electra, Haro en Heller. De tabel production.products slaat productinformatie op, zoals naam, merk, categorie, modeljaar en catalogusprijs.\nElk product behoort tot een merk dat wordt gespecificeerd door de kolom brand_id. Een merk kan dus nul of veel producten hebben.\nElk product behoort ook tot een categorie die wordt gespecificeerd door de categorie_id kolom. Elke categorie kan ook nul of veel producten hebben. De tabel sales.customers slaat klantgegevens op, waaronder voornaam, achternaam, telefoon, e-mail, straat, stad, staat en postcode. In de tabel sales.orders worden de hoofdgegevens van de verkooporder opgeslagen, waaronder de klant, de status van de order, de besteldatum, de vereiste datum en de verzenddatum.\nDe tabel slaat ook de informatie op over waar de verkooptransactie is aangemaakt (winkel) en wie deze heeft aangemaakt (personeel). In de tabel sales.order_items worden de regelitems van een verkooporder opgeslagen. Elk regelitem hoort bij een verkooporder die wordt gespecificeerd door de order_id kolom.\nEen regelitem van een verkooporder bevat het product, de bestelhoeveelheid, de catalogusprijs en de korting.\nElke verkooporder heeft een rij in de tabel verkooporders. Een verkooporder heeft een of meer regelitems die zijn opgeslagen in de tabel sales.order_items.\nDe tabel production.stocks slaat de voorraadinformatie op, d.w.z. de hoeveelheid van een bepaald product in een specifieke winkel."
  },
  {
    "objectID": "sql_course/11_exercises_subqueries.html#de-tennisvereniging",
    "href": "sql_course/11_exercises_subqueries.html#de-tennisvereniging",
    "title": "Oefeningen subqueries",
    "section": "De Tennisvereniging",
    "text": "De Tennisvereniging"
  },
  {
    "objectID": "sql_course/11_exercises_subqueries.html#subqueries-h5",
    "href": "sql_course/11_exercises_subqueries.html#subqueries-h5",
    "title": "Oefeningen subqueries",
    "section": "SUBQUERIES (H5)",
    "text": "SUBQUERIES (H5)"
  },
  {
    "objectID": "sql_course/9_exercises_joins.html#de-tennisvereniging",
    "href": "sql_course/9_exercises_joins.html#de-tennisvereniging",
    "title": "Oefeningen hoofdstuk 3: tennis joins",
    "section": "De Tennisvereniging",
    "text": "De Tennisvereniging"
  },
  {
    "objectID": "sql_course/9_exercises_joins.html#joins-h3",
    "href": "sql_course/9_exercises_joins.html#joins-h3",
    "title": "Oefeningen hoofdstuk 3: tennis joins",
    "section": "JOINS (H3)",
    "text": "JOINS (H3)"
  },
  {
    "objectID": "sql_course/8_exercises.html#de-tennisvereniging",
    "href": "sql_course/8_exercises.html#de-tennisvereniging",
    "title": "Oefeningen hoofdstuk 1 en 2: tennis",
    "section": "De Tennisvereniging",
    "text": "De Tennisvereniging"
  },
  {
    "objectID": "sql_course/8_exercises.html#basis-h1h2",
    "href": "sql_course/8_exercises.html#basis-h1h2",
    "title": "Oefeningen hoofdstuk 1 en 2: tennis",
    "section": "BASIS (H1,H2)",
    "text": "BASIS (H1,H2)"
  },
  {
    "objectID": "sql_course/index_sql.html",
    "href": "sql_course/index_sql.html",
    "title": "SQL Course",
    "section": "",
    "text": "SQL, or Structured Query Language, is a standardized programming language used to manage and manipulate relational databases. Understanding SQL opens opportunities in data analysis, data engineering, and beyond, making it an essential skill for any aspiring data professional. It enables data professionals to derive meaningful insights from large datasets, perform data analyses, and input data into their machine learning models.\nSQL was developed in the 1970s and has since become the universal language for relational database management systems (RDBMS) like MySQL, PostgreSQL, SQLite, and Microsoft SQL Server. It is widely used in data science, software development, and business intelligence for tasks such as:\nData Retrieval: Extracting specific information from large datasets using SELECT statements.\nData Manipulation: Modifying data with commands like INSERT, UPDATE, and DELETE.\nData Definition: Creating and managing database structures with CREATE, ALTER, and DROP.\nData Analysis: Aggregating and analyzing data using functions like GROUP BY, HAVING, and COUNT.\nSQLâ€™s simplicity and power make it an indispensable tool for professionals working with data, enabling efficient data management and analysis across industries."
  },
  {
    "objectID": "sql_course/index_sql.html#chapters",
    "href": "sql_course/index_sql.html#chapters",
    "title": "SQL Course",
    "section": "Chapters",
    "text": "Chapters\n\nIntroduction to SQL and Databases\nOverview of SQL, databases, and how they are used in data science.\n\n\n\nBasic SQL QueriesÂ  Learning SELECT statements and filtering data with WHERE clauses.\n\n\n\nWorking with Table Joins\nIntroduction to JOIN operations and their importance.\n\n\n\nAdvanced Data Manipulation\nUsing GROUP BY, HAVING, and aggregate functions.\n\n\n\nSubqueries, Nested Queries, and Views\nExtending queries with subqueries for more complex data retrieval.\n\n\n\nData Definition Language (DDL) & Data Manipulation Language (DML)\nUnderstanding how to create a database and insert, update, and delete data in the database.\n\n\n\nSQL and PowerBI\nExtracting insights from data by combining the power of SQL with PowerBI.\n\n\n\nExercises Chapter 1 and 2\nExercises for chapters 1 and 2.\n\n\n\nExercises Chapter 3\nExercises for chapter 3 - joins.\n\n\n\nExercises Chapter 4\nExercises for chapter 4 - GROUP BY, HAVING, etc.\n\n\n\nExercises Chapter 5\nExercises for chapter 5 - subqueries.\n\n\n\nExercises Chapter 6\nExercises for DDL - the bike shop."
  },
  {
    "objectID": "posts_main.html",
    "href": "posts_main.html",
    "title": "Articles",
    "section": "",
    "text": "Interactive Garmin Mini Competition Dashboard\n\n\n\nshiny\n\n\nfitness\n\n\ndata-visualization\n\n\ninteractive\n\n\n\nExplore fitness data through an interactive Shiny application\n\n\n\nDanny Volkaerts\n\n\nOct 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to DuckDB\n\n\n\nDuckDB\n\n\nPython\n\n\nData Science\n\n\nAnalytics\n\n\n\nA comprehensive guide to DuckDB: an analytical database designed for seamless data analysis with Python\n\n\n\nDanny Volkaerts\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIris K-Means Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Danny Volkaerts",
    "section": "",
    "text": "Hi! Welcome to my digital space! As a data enthousiast Iâ€™m fueled by curiosity and motivated by the goal of turning data into actionable insights. Whether itâ€™s through Machine Learning, data analysis, or data visualizations, I explore data in all its forms to unlock opportunities and drive innovation.\nHere, as a work in progress, I share my experiences, discoveries, and the lessons learned along the way with companies and fellow data enthusiasts. Dive into my courses, read through case studies, or reach out directly if you have any data related question!\n\n\n\n\nCatholic University of Leuven, Leuven, Belgium | Bio-engineering Sciences | Graduated 2010\n\n\n\n\n\nUC Leuven-Limburg Research & Expertise | Data Scientist | Present\nIMEC | Process Engineer | Until April 2018\n\n\n \n  \n   \n  \n    \n     LinkedIn"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Danny Volkaerts",
    "section": "",
    "text": "Catholic University of Leuven, Leuven, Belgium | Bio-engineering Sciences | Graduated 2010"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Danny Volkaerts",
    "section": "",
    "text": "UC Leuven-Limburg Research & Expertise | Data Scientist | Present\nIMEC | Process Engineer | Until April 2018"
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Courses",
    "section": "",
    "text": "Courses\n\nSQL Course: Learn the basics of SQL"
  },
  {
    "objectID": "sql_course/1_introduction.html#inleiding-tot-sql-and-databases",
    "href": "sql_course/1_introduction.html#inleiding-tot-sql-and-databases",
    "title": "Introductie SQL",
    "section": "Inleiding tot SQL and Databases",
    "text": "Inleiding tot SQL and Databases\n\nhttps://dannyvolkaerts.github.io/\nManier van werken: 1. EERD graag afdrukken of naast je houden op ander scherm 2. Interactie via Wooclap -&gt; ik ben mee met het verhaal of niet, eventueel enkele tussenvragen om te pijlen hoe ze mee zijn\nOefeningen via Azure Data Studio: New connection - Microsoft SQL Server\nExtentions links - MySQL -&gt; installeren"
  },
  {
    "objectID": "sql_course/1_introduction.html#sql-in-actie",
    "href": "sql_course/1_introduction.html#sql-in-actie",
    "title": "Introductie SQL",
    "section": "SQL in actie",
    "text": "SQL in actie"
  },
  {
    "objectID": "sql_course/7_powerbi.html#waarom-powerbi-en-sql---belang-van-data-intelligentie",
    "href": "sql_course/7_powerbi.html#waarom-powerbi-en-sql---belang-van-data-intelligentie",
    "title": "Integratie van SQL in PowerBI",
    "section": "Waarom PowerBI en SQL? - Belang van Data-intelligentie",
    "text": "Waarom PowerBI en SQL? - Belang van Data-intelligentie"
  },
  {
    "objectID": "sql_course/7_powerbi.html#concept-en-werkwijze",
    "href": "sql_course/7_powerbi.html#concept-en-werkwijze",
    "title": "Integratie van SQL in PowerBI",
    "section": "Concept en Werkwijze",
    "text": "Concept en Werkwijze"
  },
  {
    "objectID": "sql_course/7_powerbi.html#voorbeelden-van-powerbi-en-sql",
    "href": "sql_course/7_powerbi.html#voorbeelden-van-powerbi-en-sql",
    "title": "Integratie van SQL in PowerBI",
    "section": "Voorbeelden van PowerBI en SQL",
    "text": "Voorbeelden van PowerBI en SQL"
  },
  {
    "objectID": "sql_course/Tennis_uitleg.html",
    "href": "sql_course/Tennis_uitleg.html",
    "title": "De Tennisvereniging",
    "section": "",
    "text": "Database schema\n\n\n\nBelangrijke informatie bij de tennis database\nDe tennisvereniging is opgericht in 1970 en vanaf het begin wordt een aantal administratieve gegevens in een database opgeslagen. Deze database bestaat uit de volgende tabellen: spelers, teams, wedstrijden, boetes en bestuursleden.\n\nDe spelerstabel bevat gegevens over spelers die lid zijn van de tennisvereniging, zoals namen, adressen en geboortedatums. Toetreding tot de vereniging vindt altijd plaats op 1 januari van een bepaald jaar. Spelers kunnen dus niet midden in een jaar lid worden. De spelerstabel bevat geen historische gegevens. Als een speler zijn of haar lidmaatschap opzegt, verdwijnt hij of zij uit de tabel. Ook bij eventuele verhuizingen wordt het oude adres overschreven met het nieuwe adres, het oude adres wordt dus nergens bewaard.\n\nDe tennisvereniging kent twee soorten leden: recreatiespelers en wedstrijdspelers. De eerste groep speelt alleen onderlinge wedstrijden, dus geen wedstrijden tegen spelers van andere verenigingen. De resultaten van de onderlinge wedstrijden worden niet geregistreerd. Wedstrijdspelers spelen in teamverband tegen spelers van andere verenigingen. De resultaten van deze wedstrijden worden wel bijgehouden. Elke speler heeft een uniek nummer, ongeacht of deze wedstrijdspeler is of niet. Dit spelersnummer wordt door de vereniging uitgedeeld. Het is verplicht dat elke wedstrijdspeler bij de tennisbond geregistreerd staat.\n\nDe bond, die een nationaal instituut is, geeft elke wedstrijdspeler een uniek bondsnummer. Dit bondsnummer bestaat meestal uit cijfers, maar kan ook letters bevatten. Als een wedstrijdspeler geen wedstrijden meer speelt, maar recreatiespeler wordt, vervalt het bondsnummer. Let wel, recreatiespelers hebben dus geen bondsnummer, maar wel een spelersnummer.\n\nDe tennisvereniging heeft een aantal teams dat meedoet in competities. Voor elk team worden de aanvoerder en de divisie waarin het team op dat moment uitkomt, geregistreerd. De aanvoerder hoeft geen wedstrijden voor het team gespeeld te hebben. Het zou kunnen dat een bepaalde speler op een bepaald moment aanvoerder van twee of meer teams is. Ook in deze tabel wordt geen historie bijgehouden. Bij promotie of degradatie van een team naar een andere divisie wordt de geregistreerde divisie eenvoudigweg overschreven. Hetzelfde geldt voor de aanvoerder van een team: bij wisseling wordt het nummer van de oude aanvoerder overschreven.\n\nEen team bestaat uit een aantal spelers. Als een team tegen een team van een andere vereniging speelt, speelt elke speler van dat team een wedstrijd tegen een speler van het andere team (we gaan voor het gemak ervan uit dat wedstrijden waarbij koppels tegen elkaar spelen niet voorkomen). Het team waarvan de meeste spelers hun wedstrijd winnen is winnaar.\n\nEen team bestaat niet altijd uit dezelfde groep spelers. Bij ziekte of vakanties zijn soms invallers nodig. Een speler kan dus voor meerdere teams uitkomen. Als we spreken over â€˜de spelers van een teamâ€™, dan bedoelen we dus de spelers die minstens Ã©Ã©n wedstrijd voor het team gespeeld hebben. Nogmaals, alleen spelers met een bondsnummer mogen officiÃ«le wedstrijden spelen.\n\nEen tenniswedstrijd is opgebouwd uit een aantal sets. Degene die de meeste sets heeft gewonnen is winnaar. Voor elke wedstrijd wordt vooraf bepaald bij hoeveel gewonnen sets de wedstrijd gewonnen is. Over het algemeen wordt de wedstrijd gestopt als een van de twee spelers twee of drie sets gewonnen heeft. Mogelijke eindstanden van een tenniswedstrijd zijn dus 2-1 of 2-0 als gespeeld wordt totdat een van de spelers twee sets gewonnen heeft (best of three), of 3-2, 3-1 of 3-0 als gespeeld wordt tot drie gewonnen sets (best of five). Een speler kan zijn of haar wedstrijd winnen of verliezen, gelijkspel is niet mogelijk. In de wedstrijdentabel wordt voor elke wedstrijd apart bijgehouden welke speler de wedstrijd heeft gespeeld en voor welk team. Tevens wordt geregistreerd hoeveel sets de speler heeft gewonnen en verloren. Hieruit is af te leiden of hij of zij de wedstrijd gewonnen heeft.\n\nVoor onreglementair gedrag van spelers (te late opkomst, agressief gedrag of niet verschijnen) worden door de bond boetes opgelegd. Boetes worden door de vereniging betaald. Na betaling worden ze in de boetestabel geregistreerd. Zolang een speler wedstrijden speelt, blijven alle boetes bewaard die voor hem of haar opgelegd en betaald zijn.\n\nAls een speler de vereniging verlaat, worden al zijn of haar gegevens in de vijf tabellen vernietigd. Als de vereniging een team terugtrekt, worden alle gegevens over dat team uit de teams- en wedstrijdentabel verwijderd. Als een wedstrijdspeler stopt met het spelen van wedstrijden en hij of zij dus weer recreant wordt, worden alle wedstrijd- en boetegegevens uit de desbetreffende tabellen verwijderd.\n\nSinds 1 januari 1990 wordt in de bestuursledentabel bijgehouden wie er in het bestuur zitten. Vier functies worden onderscheiden: voorzitter, penningmeester, secretaris en algemeen lid. Elk jaar op 1 januari wordt een nieuw bestuur gekozen. Wanneer een speler een bestuursfunctie bekleedt, worden de begin- en einddatum hiervan geregistreerd. Als iemand nog actief is, wordt er geen einddatum ingevuld.\n\n\n\nEnkele aandachtspunten\nIn het database schema willen we even wijzen op enkele punten:\n\nMerk op dat het spelersnummer een centrale rol in het schema heeft. Het is in Ã©Ã©n tabel de primaire sleutel en in de vier andere tabellen de externe sleutel (â€˜foreign keyâ€™).\n\nDe bestuursledentabel heeft een samengestelde primaire sleutel. In de figuur zie je dat aan de twee sleutelicoontjes, in de code staan er twee kolommen in de PRIMARY KEY.\n\nEen veelgemaakte fout is geen rekening houden met het feit dat het spelersnummer in de teamstabel het nummer is van de kapitein van een team. Om te weten wie effectief wedstrijden gespeeld heeft, moet je in de wedstrijdentabel het spelersnummer gebruiken."
  },
  {
    "objectID": "sql_course/10_exercises_advanced.html#de-tennisvereniging",
    "href": "sql_course/10_exercises_advanced.html#de-tennisvereniging",
    "title": "Oefeningen advanced",
    "section": "De Tennisvereniging",
    "text": "De Tennisvereniging"
  },
  {
    "objectID": "sql_course/12_exercise_DDL.html#de-fietsenwinkel",
    "href": "sql_course/12_exercise_DDL.html#de-fietsenwinkel",
    "title": "Oefeningen DDL - Fietsenwinkel",
    "section": "De fietsenwinkel",
    "text": "De fietsenwinkel"
  },
  {
    "objectID": "sql_course/12_exercise_DDL.html#ddl",
    "href": "sql_course/12_exercise_DDL.html#ddl",
    "title": "Oefeningen DDL - Fietsenwinkel",
    "section": "DDL",
    "text": "DDL"
  },
  {
    "objectID": "posts_main/DuckDB/index.html",
    "href": "posts_main/DuckDB/index.html",
    "title": "Introduction to DuckDB",
    "section": "",
    "text": "This document provides a comprehensive overview of DuckDB, exploring its fundamental concepts, key strengths, and practical applications. Weâ€™ll demonstrate its capabilities through a detailed example using the Titanic dataset, imported directly from Kaggle using their API.\n\n\nDuckDB is a high-performance analytical database specifically engineered to operate seamlessly within your applications. By default, DuckDB operates with an in-memory database that maintains global state within the Python module when utilizing duckdb.sql(). This design choice means that no persistent tables are stored on disk unless explicitly configured otherwise.\nHowever, DuckDB also provides the flexibility to establish connections with persistent databases by specifying a filename through duckdb.connect(dbname). All data written to these connections remains persistent and can be reloaded by reconnecting to the same file, whether from Python or other DuckDB clients.\nDuckDB demonstrates remarkable versatility through its support for an extensive range of Client APIs, including: * C - Low-level systems programming\n\nC++ - High-performance applications\nCLI (Command Line Interface) - Interactive database operations\nDart - Mobile and web application development\nGo - Concurrent and scalable applications\nJava (JDBC) - Enterprise Java applications\nJulia - High-performance scientific computing\nNode.js - Server-side JavaScript applications\nODBC - Universal database connectivity\nPython - Data science and analytics workflows\nR - Statistical computing and data analysis\nRust - Systems programming with memory safety\nSwift - iOS and macOS applications\nWebAssembly (Wasm) - Browser-based applications\n\nThe database engine provides comprehensive SQL functionality, encompassing essential statements such as SELECT, INSERT, CREATE TABLE, ALTER TABLE, DELETE, UPDATE, and COPY. It includes advanced query syntax capabilities, diverse data types (Array, Boolean, Date, Numeric, Text, Timestamp), sophisticated expressions, and an extensive collection of functions including Aggregate, Date, Numeric, Text, and Window Functions.\n\n\n\nDuckDBâ€™s distinctive strengths establish it as a powerful tool for data analysis and processing, particularly when integrated with Python ecosystems:\n\n\nDuckDB excels in its ability to ingest data from a diverse array of formats, supporting both disk-based and in-memory operations. The database natively supports CSV, Parquet, and JSON file formats. One of its most compelling features is the capability to query these files directly using SQL as if they were traditional database tables. For instance, you can execute SELECT * FROM 'example.csv' without any preliminary data loading steps.\n\n\n\nDuckDB provides native support for querying Pandas DataFrames, Polars DataFrames, and PyArrow tables directly. This integration represents a significant advantage, enabling analysts to leverage SQL queries on existing Python data structures without requiring conversion or loading into separate database systems. Itâ€™s important to note that these direct queries operate in read-only mode.\n\n\n\nQuery results can be efficiently transformed into various Python objects and popular data formats. The system supports seamless conversion to Pandas DataFrames (.df()), Polars DataFrames (.pl()), Arrow Tables (.arrow()), and NumPy Arrays (.fetchnumpy()), facilitating smooth workflow integration across different data processing libraries.\n\n\n\nDuckDB offers users the choice between in-memory databases for rapid, temporary analyses and persistent storage solutions for data preservation. This flexibility allows for optimal performance tuning based on specific use case requirements.\n\n\n\nQuery results (Relation objects) can be written directly to disk in formats such as Parquet and CSV, or you can utilize the SQL COPY statement for more complex export operations. This functionality streamlines data pipeline workflows.\n\n\n\nThe duckdb module and connection objects (duckdb.connect()) support identical methods, providing implementation flexibility. For package development, connection objects are recommended to prevent potential conflicts with shared global in-memory databases.\n\n\n\nDuckDB supports the installation and loading of extensions, such as the spatial extension for geospatial data analysis or httpfs for accessing HTTP(S) and S3-based files. The platform also supports community-developed extensions through the repository=\"community\" parameter.\n\n\n\nQueries return Relation objects, which serve as symbolic representations of the query. The actual execution occurs only when results are explicitly requested or displayed, contributing to overall system efficiency through lazy evaluation.\n\n\n\n\nThis comprehensive example demonstrates how to leverage DuckDBâ€™s capabilities by importing the famous Titanic dataset directly from Kaggle using their API, then performing sophisticated data analysis using SQL queries.\n\n\nBefore proceeding, ensure you have the following requirements:\n\nPython 3.9 or newer - DuckDB requires a modern Python version\nKaggle API credentials - Youâ€™ll need a Kaggle account and API key\nRequired packages - Install via pip install duckdb kaggle pandas\n\n\n\n\nFirst, letâ€™s set up the Kaggle API to download the Titanic dataset:\n#| echo: true\n#| output: false\nimport os\nimport zipfile\nimport pandas as pd\nimport duckdb\nfrom kaggle.api.kaggle_api_extended import KaggleApi\n\n# Initialize and authenticate Kaggle API\n# Note: Make sure you have ~/.kaggle/kaggle.json with your credentials\n# or set KAGGLE_USERNAME and KAGGLE_KEY environment variables\napi = KaggleApi()\napi.authenticate()\n\n# Download the Titanic dataset from Kaggle\nprint(\"Downloading Titanic dataset from Kaggle...\")\napi.competition_download_files('titanic', path='.', quiet=False)\n\n# Extract the downloaded zip file\nwith zipfile.ZipFile('titanic.zip', 'r') as zip_ref:\n    zip_ref.extractall('.')\n\nprint(\"Dataset downloaded and extracted successfully!\")\n\n# List the extracted files\nextracted_files = [f for f in os.listdir('.') if f.endswith('.csv')]\nprint(f\"Available CSV files: {extracted_files}\")\n\n\n\nNow letâ€™s perform comprehensive analysis using DuckDBâ€™s powerful SQL capabilities:\n#| echo: true\nimport duckdb\n\n# Connect to DuckDB (in-memory database)\nconn = duckdb.connect(':memory:')\n\nprint(\"=== TITANIC DATASET ANALYSIS WITH DUCKDB ===\\n\")\n\n# Query 1: Basic dataset overview\nprint(\"1. Dataset Overview - First 5 rows:\")\noverview_query = \"\"\"\n    SELECT * \n    FROM 'train.csv' \n    LIMIT 5\n\"\"\"\nresult = conn.execute(overview_query)\nresult.df().head()\n\nprint(\"\\n2. Dataset Dimensions and Basic Statistics:\")\nstats_query = \"\"\"\n    SELECT \n        COUNT(*) as total_passengers,\n        COUNT(DISTINCT Pclass) as passenger_classes,\n        COUNT(DISTINCT Sex) as gender_categories,\n        COUNT(DISTINCT Embarked) as embarkation_ports,\n        AVG(Age) as average_age,\n        MIN(Age) as youngest_passenger,\n        MAX(Age) as oldest_passenger,\n        AVG(Fare) as average_fare\n    FROM 'train.csv'\n\"\"\"\nstats_result = conn.execute(stats_query).df()\nprint(stats_result.to_string(index=False))\n\nprint(\"\\n3. Survival Analysis by Demographics:\")\nsurvival_demographics = \"\"\"\n    SELECT \n        Sex,\n        Pclass,\n        COUNT(*) as total_passengers,\n        SUM(Survived) as survivors,\n        ROUND(AVG(Survived) * 100, 2) as survival_rate_percent,\n        ROUND(AVG(Age), 1) as avg_age,\n        ROUND(AVG(Fare), 2) as avg_fare\n    FROM 'train.csv'\n    WHERE Age IS NOT NULL\n    GROUP BY Sex, Pclass\n    ORDER BY Sex, Pclass\n\"\"\"\ndemographics_result = conn.execute(survival_demographics).df()\nprint(demographics_result.to_string(index=False))\n\nprint(\"\\n4. Advanced Analysis: Survival by Age Groups:\")\nage_group_analysis = \"\"\"\n    SELECT \n        CASE \n            WHEN Age &lt; 18 THEN 'Child (0-17)'\n            WHEN Age &gt;= 18 AND Age &lt; 35 THEN 'Young Adult (18-34)'\n            WHEN Age &gt;= 35 AND Age &lt; 55 THEN 'Middle Age (35-54)'\n            WHEN Age &gt;= 55 THEN 'Senior (55+)'\n            ELSE 'Unknown'\n        END as age_group,\n        COUNT(*) as total_passengers,\n        SUM(Survived) as survivors,\n        ROUND(AVG(Survived) * 100, 2) as survival_rate_percent,\n        ROUND(AVG(Fare), 2) as avg_fare_paid\n    FROM 'train.csv'\n    WHERE Age IS NOT NULL\n    GROUP BY age_group\n    ORDER BY \n        CASE \n            WHEN age_group = 'Child (0-17)' THEN 1\n            WHEN age_group = 'Young Adult (18-34)' THEN 2\n            WHEN age_group = 'Middle Age (35-54)' THEN 3\n            WHEN age_group = 'Senior (55+)' THEN 4\n            ELSE 5\n        END\n\"\"\"\nage_analysis_result = conn.execute(age_group_analysis).df()\nprint(age_analysis_result.to_string(index=False))\n\nprint(\"\\n5. Family Size Impact on Survival:\")\nfamily_analysis = \"\"\"\n    SELECT \n        (SibSp + Parch) as family_size,\n        CASE \n            WHEN (SibSp + Parch) = 0 THEN 'Alone'\n            WHEN (SibSp + Parch) BETWEEN 1 AND 3 THEN 'Small Family (1-3)'\n            WHEN (SibSp + Parch) BETWEEN 4 AND 6 THEN 'Medium Family (4-6)'\n            ELSE 'Large Family (7+)'\n        END as family_category,\n        COUNT(*) as passenger_count,\n        SUM(Survived) as survivors,\n        ROUND(AVG(Survived) * 100, 2) as survival_rate_percent\n    FROM 'train.csv'\n    GROUP BY family_size, family_category\n    ORDER BY family_size\n\"\"\"\nfamily_result = conn.execute(family_analysis).df()\nprint(family_result.to_string(index=False))\n\nprint(\"\\n6. Port of Embarkation Analysis:\")\nembarkation_analysis = \"\"\"\n    SELECT \n        CASE \n            WHEN Embarked = 'C' THEN 'Cherbourg'\n            WHEN Embarked = 'Q' THEN 'Queenstown'\n            WHEN Embarked = 'S' THEN 'Southampton'\n            ELSE 'Unknown'\n        END as port_of_embarkation,\n        COUNT(*) as passenger_count,\n        SUM(Survived) as survivors,\n        ROUND(AVG(Survived) * 100, 2) as survival_rate_percent,\n        ROUND(AVG(Fare), 2) as avg_fare,\n        ROUND(AVG(Age), 1) as avg_age\n    FROM 'train.csv'\n    WHERE Embarked IS NOT NULL\n    GROUP BY Embarked, port_of_embarkation\n    ORDER BY survival_rate_percent DESC\n\"\"\"\nembarkation_result = conn.execute(embarkation_analysis).df()\nprint(embarkation_result.to_string(index=False))\n\n# Demonstrate DuckDB's ability to export results\nprint(\"\\n7. Exporting Analysis Results:\")\n# Export the demographic analysis to a new CSV file\nconn.execute(\"\"\"\n    COPY (\n        SELECT \n            Sex,\n            Pclass,\n            COUNT(*) as total_passengers,\n            SUM(Survived) as survivors,\n            ROUND(AVG(Survived) * 100, 2) as survival_rate_percent\n        FROM 'train.csv'\n        GROUP BY Sex, Pclass\n        ORDER BY Sex, Pclass\n    ) TO 'titanic_survival_analysis.csv' (HEADER, DELIMITER ',')\n\"\"\")\nprint(\"Results exported to 'titanic_survival_analysis.csv'\")\n\n# Demonstrate conversion to different Python data structures\nprint(\"\\n8. Converting Results to Different Data Structures:\")\nsample_result = conn.execute(\"SELECT * FROM 'train.csv' LIMIT 3\")\n\n# Convert to Pandas DataFrame\npandas_df = sample_result.df()\nprint(f\"Pandas DataFrame shape: {pandas_df.shape}\")\n\n# Convert to Arrow Table\narrow_table = sample_result.arrow()\nprint(f\"Arrow Table schema: {arrow_table.schema}\")\n\n# Convert to NumPy array (for numerical columns only)\nnumeric_result = conn.execute(\"SELECT Age, Fare FROM 'train.csv' WHERE Age IS NOT NULL LIMIT 5\")\nnumpy_array = numeric_result.fetchnumpy()\nprint(f\"NumPy arrays: {list(numpy_array.keys())}\")\n\n# Close the connection\nconn.close()\nprint(\"\\nAnalysis complete! DuckDB connection closed.\")\n\n\n\nThis example demonstrates several powerful DuckDB features:\n\nDirect CSV querying without loading data into memory first\nComplex SQL analytics with window functions and case statements\nEfficient data type handling and NULL value management\nSeamless integration with Python data science ecosystem\nMultiple export formats and data structure conversions\nPerformance optimization through lazy evaluation\n\nThe combination of Kaggleâ€™s API and DuckDBâ€™s analytical capabilities showcases how modern data science workflows can be streamlined and made more efficient. DuckDBâ€™s SQL-first approach allows for complex analytical queries while maintaining the flexibility to integrate with existing Python data science libraries.\n\n\n\n#| echo: true\n#| output: false\n# Clean up downloaded files (optional)\nimport os\nfiles_to_remove = ['titanic.zip', 'train.csv', 'test.csv', 'gender_submission.csv', 'titanic_survival_analysis.csv']\nfor file in files_to_remove:\n    if os.path.exists(file):\n        os.remove(file)\n        print(f\"Removed: {file}\")\nThis comprehensive example illustrates how DuckDB can serve as a powerful analytical engine for data science workflows, combining the familiarity of SQL with the flexibility of Python data processing libraries."
  },
  {
    "objectID": "posts_main/DuckDB/index.html#what-is-duckdb",
    "href": "posts_main/DuckDB/index.html#what-is-duckdb",
    "title": "Introduction to DuckDB",
    "section": "",
    "text": "DuckDB is a high-performance analytical database specifically engineered to operate seamlessly within your applications. By default, DuckDB operates with an in-memory database that maintains global state within the Python module when utilizing duckdb.sql(). This design choice means that no persistent tables are stored on disk unless explicitly configured otherwise.\nHowever, DuckDB also provides the flexibility to establish connections with persistent databases by specifying a filename through duckdb.connect(dbname). All data written to these connections remains persistent and can be reloaded by reconnecting to the same file, whether from Python or other DuckDB clients.\nDuckDB demonstrates remarkable versatility through its support for an extensive range of Client APIs, including: * C - Low-level systems programming\n\nC++ - High-performance applications\nCLI (Command Line Interface) - Interactive database operations\nDart - Mobile and web application development\nGo - Concurrent and scalable applications\nJava (JDBC) - Enterprise Java applications\nJulia - High-performance scientific computing\nNode.js - Server-side JavaScript applications\nODBC - Universal database connectivity\nPython - Data science and analytics workflows\nR - Statistical computing and data analysis\nRust - Systems programming with memory safety\nSwift - iOS and macOS applications\nWebAssembly (Wasm) - Browser-based applications\n\nThe database engine provides comprehensive SQL functionality, encompassing essential statements such as SELECT, INSERT, CREATE TABLE, ALTER TABLE, DELETE, UPDATE, and COPY. It includes advanced query syntax capabilities, diverse data types (Array, Boolean, Date, Numeric, Text, Timestamp), sophisticated expressions, and an extensive collection of functions including Aggregate, Date, Numeric, Text, and Window Functions."
  },
  {
    "objectID": "posts_main/DuckDB/index.html#what-makes-duckdb-exceptional",
    "href": "posts_main/DuckDB/index.html#what-makes-duckdb-exceptional",
    "title": "Introduction to DuckDB",
    "section": "",
    "text": "DuckDBâ€™s distinctive strengths establish it as a powerful tool for data analysis and processing, particularly when integrated with Python ecosystems:\n\n\nDuckDB excels in its ability to ingest data from a diverse array of formats, supporting both disk-based and in-memory operations. The database natively supports CSV, Parquet, and JSON file formats. One of its most compelling features is the capability to query these files directly using SQL as if they were traditional database tables. For instance, you can execute SELECT * FROM 'example.csv' without any preliminary data loading steps.\n\n\n\nDuckDB provides native support for querying Pandas DataFrames, Polars DataFrames, and PyArrow tables directly. This integration represents a significant advantage, enabling analysts to leverage SQL queries on existing Python data structures without requiring conversion or loading into separate database systems. Itâ€™s important to note that these direct queries operate in read-only mode.\n\n\n\nQuery results can be efficiently transformed into various Python objects and popular data formats. The system supports seamless conversion to Pandas DataFrames (.df()), Polars DataFrames (.pl()), Arrow Tables (.arrow()), and NumPy Arrays (.fetchnumpy()), facilitating smooth workflow integration across different data processing libraries.\n\n\n\nDuckDB offers users the choice between in-memory databases for rapid, temporary analyses and persistent storage solutions for data preservation. This flexibility allows for optimal performance tuning based on specific use case requirements.\n\n\n\nQuery results (Relation objects) can be written directly to disk in formats such as Parquet and CSV, or you can utilize the SQL COPY statement for more complex export operations. This functionality streamlines data pipeline workflows.\n\n\n\nThe duckdb module and connection objects (duckdb.connect()) support identical methods, providing implementation flexibility. For package development, connection objects are recommended to prevent potential conflicts with shared global in-memory databases.\n\n\n\nDuckDB supports the installation and loading of extensions, such as the spatial extension for geospatial data analysis or httpfs for accessing HTTP(S) and S3-based files. The platform also supports community-developed extensions through the repository=\"community\" parameter.\n\n\n\nQueries return Relation objects, which serve as symbolic representations of the query. The actual execution occurs only when results are explicitly requested or displayed, contributing to overall system efficiency through lazy evaluation."
  },
  {
    "objectID": "posts_main/DuckDB/index.html#practical-example-analyzing-the-titanic-dataset-with-duckdb-and-kaggle-api",
    "href": "posts_main/DuckDB/index.html#practical-example-analyzing-the-titanic-dataset-with-duckdb-and-kaggle-api",
    "title": "Introduction to DuckDB",
    "section": "",
    "text": "This comprehensive example demonstrates how to leverage DuckDBâ€™s capabilities by importing the famous Titanic dataset directly from Kaggle using their API, then performing sophisticated data analysis using SQL queries.\n\n\nBefore proceeding, ensure you have the following requirements:\n\nPython 3.9 or newer - DuckDB requires a modern Python version\nKaggle API credentials - Youâ€™ll need a Kaggle account and API key\nRequired packages - Install via pip install duckdb kaggle pandas\n\n\n\n\nFirst, letâ€™s set up the Kaggle API to download the Titanic dataset:\n#| echo: true\n#| output: false\nimport os\nimport zipfile\nimport pandas as pd\nimport duckdb\nfrom kaggle.api.kaggle_api_extended import KaggleApi\n\n# Initialize and authenticate Kaggle API\n# Note: Make sure you have ~/.kaggle/kaggle.json with your credentials\n# or set KAGGLE_USERNAME and KAGGLE_KEY environment variables\napi = KaggleApi()\napi.authenticate()\n\n# Download the Titanic dataset from Kaggle\nprint(\"Downloading Titanic dataset from Kaggle...\")\napi.competition_download_files('titanic', path='.', quiet=False)\n\n# Extract the downloaded zip file\nwith zipfile.ZipFile('titanic.zip', 'r') as zip_ref:\n    zip_ref.extractall('.')\n\nprint(\"Dataset downloaded and extracted successfully!\")\n\n# List the extracted files\nextracted_files = [f for f in os.listdir('.') if f.endswith('.csv')]\nprint(f\"Available CSV files: {extracted_files}\")\n\n\n\nNow letâ€™s perform comprehensive analysis using DuckDBâ€™s powerful SQL capabilities:\n#| echo: true\nimport duckdb\n\n# Connect to DuckDB (in-memory database)\nconn = duckdb.connect(':memory:')\n\nprint(\"=== TITANIC DATASET ANALYSIS WITH DUCKDB ===\\n\")\n\n# Query 1: Basic dataset overview\nprint(\"1. Dataset Overview - First 5 rows:\")\noverview_query = \"\"\"\n    SELECT * \n    FROM 'train.csv' \n    LIMIT 5\n\"\"\"\nresult = conn.execute(overview_query)\nresult.df().head()\n\nprint(\"\\n2. Dataset Dimensions and Basic Statistics:\")\nstats_query = \"\"\"\n    SELECT \n        COUNT(*) as total_passengers,\n        COUNT(DISTINCT Pclass) as passenger_classes,\n        COUNT(DISTINCT Sex) as gender_categories,\n        COUNT(DISTINCT Embarked) as embarkation_ports,\n        AVG(Age) as average_age,\n        MIN(Age) as youngest_passenger,\n        MAX(Age) as oldest_passenger,\n        AVG(Fare) as average_fare\n    FROM 'train.csv'\n\"\"\"\nstats_result = conn.execute(stats_query).df()\nprint(stats_result.to_string(index=False))\n\nprint(\"\\n3. Survival Analysis by Demographics:\")\nsurvival_demographics = \"\"\"\n    SELECT \n        Sex,\n        Pclass,\n        COUNT(*) as total_passengers,\n        SUM(Survived) as survivors,\n        ROUND(AVG(Survived) * 100, 2) as survival_rate_percent,\n        ROUND(AVG(Age), 1) as avg_age,\n        ROUND(AVG(Fare), 2) as avg_fare\n    FROM 'train.csv'\n    WHERE Age IS NOT NULL\n    GROUP BY Sex, Pclass\n    ORDER BY Sex, Pclass\n\"\"\"\ndemographics_result = conn.execute(survival_demographics).df()\nprint(demographics_result.to_string(index=False))\n\nprint(\"\\n4. Advanced Analysis: Survival by Age Groups:\")\nage_group_analysis = \"\"\"\n    SELECT \n        CASE \n            WHEN Age &lt; 18 THEN 'Child (0-17)'\n            WHEN Age &gt;= 18 AND Age &lt; 35 THEN 'Young Adult (18-34)'\n            WHEN Age &gt;= 35 AND Age &lt; 55 THEN 'Middle Age (35-54)'\n            WHEN Age &gt;= 55 THEN 'Senior (55+)'\n            ELSE 'Unknown'\n        END as age_group,\n        COUNT(*) as total_passengers,\n        SUM(Survived) as survivors,\n        ROUND(AVG(Survived) * 100, 2) as survival_rate_percent,\n        ROUND(AVG(Fare), 2) as avg_fare_paid\n    FROM 'train.csv'\n    WHERE Age IS NOT NULL\n    GROUP BY age_group\n    ORDER BY \n        CASE \n            WHEN age_group = 'Child (0-17)' THEN 1\n            WHEN age_group = 'Young Adult (18-34)' THEN 2\n            WHEN age_group = 'Middle Age (35-54)' THEN 3\n            WHEN age_group = 'Senior (55+)' THEN 4\n            ELSE 5\n        END\n\"\"\"\nage_analysis_result = conn.execute(age_group_analysis).df()\nprint(age_analysis_result.to_string(index=False))\n\nprint(\"\\n5. Family Size Impact on Survival:\")\nfamily_analysis = \"\"\"\n    SELECT \n        (SibSp + Parch) as family_size,\n        CASE \n            WHEN (SibSp + Parch) = 0 THEN 'Alone'\n            WHEN (SibSp + Parch) BETWEEN 1 AND 3 THEN 'Small Family (1-3)'\n            WHEN (SibSp + Parch) BETWEEN 4 AND 6 THEN 'Medium Family (4-6)'\n            ELSE 'Large Family (7+)'\n        END as family_category,\n        COUNT(*) as passenger_count,\n        SUM(Survived) as survivors,\n        ROUND(AVG(Survived) * 100, 2) as survival_rate_percent\n    FROM 'train.csv'\n    GROUP BY family_size, family_category\n    ORDER BY family_size\n\"\"\"\nfamily_result = conn.execute(family_analysis).df()\nprint(family_result.to_string(index=False))\n\nprint(\"\\n6. Port of Embarkation Analysis:\")\nembarkation_analysis = \"\"\"\n    SELECT \n        CASE \n            WHEN Embarked = 'C' THEN 'Cherbourg'\n            WHEN Embarked = 'Q' THEN 'Queenstown'\n            WHEN Embarked = 'S' THEN 'Southampton'\n            ELSE 'Unknown'\n        END as port_of_embarkation,\n        COUNT(*) as passenger_count,\n        SUM(Survived) as survivors,\n        ROUND(AVG(Survived) * 100, 2) as survival_rate_percent,\n        ROUND(AVG(Fare), 2) as avg_fare,\n        ROUND(AVG(Age), 1) as avg_age\n    FROM 'train.csv'\n    WHERE Embarked IS NOT NULL\n    GROUP BY Embarked, port_of_embarkation\n    ORDER BY survival_rate_percent DESC\n\"\"\"\nembarkation_result = conn.execute(embarkation_analysis).df()\nprint(embarkation_result.to_string(index=False))\n\n# Demonstrate DuckDB's ability to export results\nprint(\"\\n7. Exporting Analysis Results:\")\n# Export the demographic analysis to a new CSV file\nconn.execute(\"\"\"\n    COPY (\n        SELECT \n            Sex,\n            Pclass,\n            COUNT(*) as total_passengers,\n            SUM(Survived) as survivors,\n            ROUND(AVG(Survived) * 100, 2) as survival_rate_percent\n        FROM 'train.csv'\n        GROUP BY Sex, Pclass\n        ORDER BY Sex, Pclass\n    ) TO 'titanic_survival_analysis.csv' (HEADER, DELIMITER ',')\n\"\"\")\nprint(\"Results exported to 'titanic_survival_analysis.csv'\")\n\n# Demonstrate conversion to different Python data structures\nprint(\"\\n8. Converting Results to Different Data Structures:\")\nsample_result = conn.execute(\"SELECT * FROM 'train.csv' LIMIT 3\")\n\n# Convert to Pandas DataFrame\npandas_df = sample_result.df()\nprint(f\"Pandas DataFrame shape: {pandas_df.shape}\")\n\n# Convert to Arrow Table\narrow_table = sample_result.arrow()\nprint(f\"Arrow Table schema: {arrow_table.schema}\")\n\n# Convert to NumPy array (for numerical columns only)\nnumeric_result = conn.execute(\"SELECT Age, Fare FROM 'train.csv' WHERE Age IS NOT NULL LIMIT 5\")\nnumpy_array = numeric_result.fetchnumpy()\nprint(f\"NumPy arrays: {list(numpy_array.keys())}\")\n\n# Close the connection\nconn.close()\nprint(\"\\nAnalysis complete! DuckDB connection closed.\")\n\n\n\nThis example demonstrates several powerful DuckDB features:\n\nDirect CSV querying without loading data into memory first\nComplex SQL analytics with window functions and case statements\nEfficient data type handling and NULL value management\nSeamless integration with Python data science ecosystem\nMultiple export formats and data structure conversions\nPerformance optimization through lazy evaluation\n\nThe combination of Kaggleâ€™s API and DuckDBâ€™s analytical capabilities showcases how modern data science workflows can be streamlined and made more efficient. DuckDBâ€™s SQL-first approach allows for complex analytical queries while maintaining the flexibility to integrate with existing Python data science libraries.\n\n\n\n#| echo: true\n#| output: false\n# Clean up downloaded files (optional)\nimport os\nfiles_to_remove = ['titanic.zip', 'train.csv', 'test.csv', 'gender_submission.csv', 'titanic_survival_analysis.csv']\nfor file in files_to_remove:\n    if os.path.exists(file):\n        os.remove(file)\n        print(f\"Removed: {file}\")\nThis comprehensive example illustrates how DuckDB can serve as a powerful analytical engine for data science workflows, combining the familiarity of SQL with the flexibility of Python data processing libraries."
  },
  {
    "objectID": "posts_main/Iris_Kmeans_Clustering/index.html",
    "href": "posts_main/Iris_Kmeans_Clustering/index.html",
    "title": "Iris K-Means Clustering",
    "section": "",
    "text": "This analysis demonstrates k-means clustering on the famous iris dataset. Weâ€™ll explore how different variables cluster together and visualize the results with various cluster counts.\n\n\nCode\nlibrary(ggplot2)\n\n\nWarning: package 'ggplot2' was built under R version 4.5.1\n\n\nCode\nlibrary(dplyr)\nlibrary(patchwork)\n\n# Load the iris dataset\ndata(iris)"
  },
  {
    "objectID": "posts_main/Iris_Kmeans_Clustering/index.html#introduction",
    "href": "posts_main/Iris_Kmeans_Clustering/index.html#introduction",
    "title": "Iris K-Means Clustering",
    "section": "",
    "text": "This analysis demonstrates k-means clustering on the famous iris dataset. Weâ€™ll explore how different variables cluster together and visualize the results with various cluster counts.\n\n\nCode\nlibrary(ggplot2)\n\n\nWarning: package 'ggplot2' was built under R version 4.5.1\n\n\nCode\nlibrary(dplyr)\nlibrary(patchwork)\n\n# Load the iris dataset\ndata(iris)"
  },
  {
    "objectID": "posts_main/Iris_Kmeans_Clustering/index.html#data-overview",
    "href": "posts_main/Iris_Kmeans_Clustering/index.html#data-overview",
    "title": "Iris K-Means Clustering",
    "section": "Data Overview",
    "text": "Data Overview\nThe iris dataset contains measurements for 150 flowers from three species of iris:\n\n\nCode\n# Display structure of the data\nstr(iris)\n\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nCode\n# Summary statistics\nsummary(iris)\n\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50"
  },
  {
    "objectID": "posts_main/Iris_Kmeans_Clustering/index.html#k-means-clustering-analysis",
    "href": "posts_main/Iris_Kmeans_Clustering/index.html#k-means-clustering-analysis",
    "title": "Iris K-Means Clustering",
    "section": "K-Means Clustering Analysis",
    "text": "K-Means Clustering Analysis\nLetâ€™s perform k-means clustering using different variable combinations and cluster counts.\n\nSepal Length vs Sepal Width\n\n\nCode\n# Prepare data for clustering (Sepal Length vs Sepal Width)\nsepal_data &lt;- iris[, c(\"Sepal.Length\", \"Sepal.Width\")]\n\n# Perform k-means with different cluster counts\nset.seed(123)\nk2_sepal &lt;- kmeans(sepal_data, centers = 2)\nk3_sepal &lt;- kmeans(sepal_data, centers = 3)\nk4_sepal &lt;- kmeans(sepal_data, centers = 4)\n\n# Create plotting function\ncreate_cluster_plot &lt;- function(data, clusters, title, x_var, y_var) {\n  # Create data frame with cluster assignments\n  plot_data &lt;- data.frame(\n    x = data[, 1],\n    y = data[, 2],\n    cluster = as.factor(clusters$cluster),\n    species = iris$Species\n  )\n  \n  # Create centers data frame\n  centers_data &lt;- data.frame(\n    x = clusters$centers[, 1],\n    y = clusters$centers[, 2]\n  )\n  \n  ggplot(plot_data, aes(x = x, y = y)) +\n    geom_point(aes(color = cluster, shape = species), size = 3, alpha = 0.7) +\n    geom_point(data = centers_data, aes(x = x, y = y), \n               color = \"black\", shape = 4, size = 4, stroke = 2) +\n    labs(title = title,\n         x = x_var,\n         y = y_var,\n         color = \"Cluster\",\n         shape = \"Species\") +\n    theme_minimal() +\n    scale_color_brewer(type = \"qual\", palette = \"Set1\")\n}\n\n# Create plots for different cluster counts\np1 &lt;- create_cluster_plot(sepal_data, k2_sepal, \"K=2\", \"Sepal Length\", \"Sepal Width\")\np2 &lt;- create_cluster_plot(sepal_data, k3_sepal, \"K=3\", \"Sepal Length\", \"Sepal Width\")\np3 &lt;- create_cluster_plot(sepal_data, k4_sepal, \"K=4\", \"Sepal Length\", \"Sepal Width\")\n\n# Combine plots\np1 / p2 / p3\n\n\n\n\n\n\n\n\n\n\n\nPetal Length vs Petal Width\n\n\nCode\n# Prepare data for clustering (Petal Length vs Petal Width)\npetal_data &lt;- iris[, c(\"Petal.Length\", \"Petal.Width\")]\n\n# Perform k-means with different cluster counts\nset.seed(123)\nk2_petal &lt;- kmeans(petal_data, centers = 2)\nk3_petal &lt;- kmeans(petal_data, centers = 3)\nk4_petal &lt;- kmeans(petal_data, centers = 4)\n\n# Create plots for different cluster counts\np4 &lt;- create_cluster_plot(petal_data, k2_petal, \"K=2\", \"Petal Length\", \"Petal Width\")\np5 &lt;- create_cluster_plot(petal_data, k3_petal, \"K=3\", \"Petal Length\", \"Petal Width\")\np6 &lt;- create_cluster_plot(petal_data, k4_petal, \"K=4\", \"Petal Length\", \"Petal Width\")\n\n# Combine plots\np4 / p5 / p6"
  },
  {
    "objectID": "posts_main/Iris_Kmeans_Clustering/index.html#cluster-quality-assessment",
    "href": "posts_main/Iris_Kmeans_Clustering/index.html#cluster-quality-assessment",
    "title": "Iris K-Means Clustering",
    "section": "Cluster Quality Assessment",
    "text": "Cluster Quality Assessment\nLetâ€™s evaluate how well our clusters match the actual species:\n\n\nCode\n# Function to calculate cluster purity\ncalculate_purity &lt;- function(clusters, species) {\n  cluster_species &lt;- table(clusters$cluster, species)\n  purity &lt;- sum(apply(cluster_species, 1, max)) / sum(cluster_species)\n  return(purity)\n}\n\n# Calculate purity for different analyses\nsepal_purity &lt;- data.frame(\n  k = c(2, 3, 4),\n  purity = c(\n    calculate_purity(k2_sepal, iris$Species),\n    calculate_purity(k3_sepal, iris$Species),\n    calculate_purity(k4_sepal, iris$Species)\n  ),\n  variables = \"Sepal Length vs Width\"\n)\n\npetal_purity &lt;- data.frame(\n  k = c(2, 3, 4),\n  purity = c(\n    calculate_purity(k2_petal, iris$Species),\n    calculate_purity(k3_petal, iris$Species),\n    calculate_purity(k4_petal, iris$Species)\n  ),\n  variables = \"Petal Length vs Width\"\n)\n\n# Combine and display results\npurity_results &lt;- rbind(sepal_purity, petal_purity)\nprint(purity_results)\n\n\n  k    purity             variables\n1 2 0.6466667 Sepal Length vs Width\n2 3 0.8200000 Sepal Length vs Width\n3 4 0.7800000 Sepal Length vs Width\n4 2 0.6666667 Petal Length vs Width\n5 3 0.9600000 Petal Length vs Width\n6 4 0.9400000 Petal Length vs Width\n\n\nCode\n# Plot purity comparison\nggplot(purity_results, aes(x = k, y = purity, color = variables)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  labs(title = \"Cluster Purity by Variable Combination\",\n       x = \"Number of Clusters (k)\",\n       y = \"Purity Score\",\n       color = \"Variables\") +\n  theme_minimal() +\n  scale_x_continuous(breaks = c(2, 3, 4)) +\n  scale_y_continuous(limits = c(0, 1))\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nâ„¹ Please use `linewidth` instead."
  },
  {
    "objectID": "posts_main/Iris_Kmeans_Clustering/index.html#within-cluster-sum-of-squares-wcss",
    "href": "posts_main/Iris_Kmeans_Clustering/index.html#within-cluster-sum-of-squares-wcss",
    "title": "Iris K-Means Clustering",
    "section": "Within-Cluster Sum of Squares (WCSS)",
    "text": "Within-Cluster Sum of Squares (WCSS)\n\n\nCode\n# Function to calculate WCSS for different k values\ncalculate_wcss &lt;- function(data, max_k = 10) {\n  wcss &lt;- numeric(max_k)\n  for (i in 1:max_k) {\n    set.seed(123)\n    kmeans_result &lt;- kmeans(data, centers = i)\n    wcss[i] &lt;- kmeans_result$tot.withinss\n  }\n  return(wcss)\n}\n\n# Calculate WCSS for both variable combinations\nwcss_sepal &lt;- calculate_wcss(sepal_data)\nwcss_petal &lt;- calculate_wcss(petal_data)\n\n# Create elbow plot\nelbow_data &lt;- data.frame(\n  k = rep(1:10, 2),\n  wcss = c(wcss_sepal, wcss_petal),\n  variables = rep(c(\"Sepal Length vs Width\", \"Petal Length vs Width\"), each = 10)\n)\n\nggplot(elbow_data, aes(x = k, y = wcss, color = variables)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  labs(title = \"Elbow Method for Optimal k\",\n       x = \"Number of Clusters (k)\",\n       y = \"Within-Cluster Sum of Squares\",\n       color = \"Variables\") +\n  theme_minimal() +\n  scale_x_continuous(breaks = 1:10)"
  },
  {
    "objectID": "posts_main/Iris_Kmeans_Clustering/index.html#conclusions",
    "href": "posts_main/Iris_Kmeans_Clustering/index.html#conclusions",
    "title": "Iris K-Means Clustering",
    "section": "Conclusions",
    "text": "Conclusions\n\nPetal measurements show much clearer clustering patterns than sepal measurements\nThe optimal number of clusters appears to be k=3, which matches the number of species\nPetal length vs width achieves higher purity scores, indicating better separation between species\nThe elbow method suggests k=3 as the optimal number of clusters for both variable combinations"
  }
]